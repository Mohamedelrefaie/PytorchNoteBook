{
  
    
        "post0": {
            "title": "fastai datacore",
            "content": "! [ -e /content ] &amp;&amp; pip install -Uqq fastai # upgrade fastai on colab . from fastai.torch_basics import * from fastai.data.load import * from fastai.vision.all import * . data_path = Path(&#39;/home/ubuntu/.fastai/data/isprs/&#39;) data_path.ls() # path_img = data_path/&#39;2_Ortho_RGB&#39; # path_lbl = data_path/&#39;5_Labels_for_participants&#39; path_img = data_path / &#39;Potsdam/2_Ortho_RGB/train_pick&#39; path_lbl = data_path / &#39;Potsdam/5_Labels_for_participants&#39; fnames = get_image_files(path_img) fnames[:3] lbl_names = get_image_files(path_lbl) lbl_names[:3] . (#7) [Path(&#39;/home/ubuntu/.fastai/data/isprs/5_Labels_for_participants.zip&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/4_Ortho_RGBIR.zip&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/haze&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/2_Ortho_RGB.zip&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Vaihingen&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/bak&#39;)] . (#3) [Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/2_Ortho_RGB/train_pick/top_potsdam_7_9_RGB.tif&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/2_Ortho_RGB/train_pick/top_potsdam_5_10_RGB.tif&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/2_Ortho_RGB/train_pick/top_potsdam_5_11_RGB.tif&#39;)] . (#3) [Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/5_Labels_for_participants/top_potsdam_7_7_label.tif&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/5_Labels_for_participants/top_potsdam_2_10_label.tif&#39;),Path(&#39;/home/ubuntu/.fastai/data/isprs/Potsdam/5_Labels_for_participants/top_potsdam_6_8_label.tif&#39;)] . Data core . Core functionality for gathering data . The classes here provide functionality for applying a list of transforms to a set of items (TfmdLists, Datasets) or a DataLoader (TfmdDl) as well as the base class used to gather the data for model training: DataLoaders. . @typedispatch def show_batch(x, y, samples, ctxs=None, max_n=9, **kwargs): if ctxs is None: ctxs = Inf.nones if hasattr(samples[0], &#39;show&#39;): ctxs = [s.show(ctx=c, **kwargs) for s,c,_ in zip(samples,ctxs,range(max_n))] else: for i in range_of(samples[0]): ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))] return ctxs . show_batch is a type-dispatched function that is responsible for showing decoded samples. x and y are the input and the target in the batch to be shown, and are passed along to dispatch on their types. There is a different implementation of show_batch if x is a TensorImage or a TensorText for instance (see vision.core or text.data for more details). ctxs can be passed but the function is responsible to create them if necessary. kwargs depend on the specific implementation. . @typedispatch def show_results(x, y, samples, outs, ctxs=None, max_n=9, **kwargs): if ctxs is None: ctxs = Inf.nones for i in range(len(samples[0])): ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs,range(max_n))] for i in range(len(outs[0])): ctxs = [b.show(ctx=c, **kwargs) for b,c,_ in zip(outs.itemgot(i),ctxs,range(max_n))] return ctxs . show_results is a type-dispatched function that is responsible for showing decoded samples and their corresponding outs. Like in show_batch, x and y are the input and the target in the batch to be shown, and are passed along to dispatch on their types. ctxs can be passed but the function is responsible to create them if necessary. kwargs depend on the specific implementation. . _all_ = [&quot;show_batch&quot;, &quot;show_results&quot;] . _batch_tfms = (&#39;after_item&#39;,&#39;before_batch&#39;,&#39;after_batch&#39;) . @delegates() class TfmdDL(DataLoader): &quot;Transformed `DataLoader`&quot; def __init__(self, dataset, bs=64, shuffle=False, num_workers=None, verbose=False, do_setup=True, **kwargs): if num_workers is None: num_workers = min(16, defaults.cpus) for nm in _batch_tfms: kwargs[nm] = Pipeline(kwargs.get(nm,None)) super().__init__(dataset, bs=bs, shuffle=shuffle, num_workers=num_workers, **kwargs) if do_setup: for nm in _batch_tfms: pv(f&quot;Setting up {nm}: {kwargs[nm]}&quot;, verbose) kwargs[nm].setup(self) def _one_pass(self): b = self.do_batch([self.do_item(0)]) if self.device is not None: b = to_device(b, self.device) its = self.after_batch(b) self._n_inp = 1 if not isinstance(its, (list,tuple)) or len(its)==1 else len(its)-1 self._types = explode_types(its) def _retain_dl(self,b): if not getattr(self, &#39;_types&#39;, None): self._one_pass() return retain_types(b, typs=self._types) @delegates(DataLoader.new) def new(self, dataset=None, cls=None, **kwargs): res = super().new(dataset, cls, do_setup=False, **kwargs) if not hasattr(self, &#39;_n_inp&#39;) or not hasattr(self, &#39;_types&#39;): try: self._one_pass() res._n_inp,res._types = self._n_inp,self._types except: print(&quot;Could not do one pass in your dataloader, there is something wrong in it&quot;) else: res._n_inp,res._types = self._n_inp,self._types return res def before_iter(self): super().before_iter() split_idx = getattr(self.dataset, &#39;split_idx&#39;, None) for nm in _batch_tfms: f = getattr(self,nm) if isinstance(f,Pipeline): f.split_idx=split_idx def decode(self, b): return to_cpu(self.after_batch.decode(self._retain_dl(b))) def decode_batch(self, b, max_n=9, full=True): return self._decode_batch(self.decode(b), max_n, full) def _decode_batch(self, b, max_n=9, full=True): f = self.after_item.decode f1 = self.before_batch.decode f = compose(f1, f, partial(getattr(self.dataset,&#39;decode&#39;,noop), full = full)) return L(batch_to_samples(b, max_n=max_n)).map(f) def _pre_show_batch(self, b, max_n=9): &quot;Decode `b` to be ready for `show_batch`&quot; b = self.decode(b) if hasattr(b, &#39;show&#39;): return b,None,None its = self._decode_batch(b, max_n, full=False) if not is_listy(b): b,its = [b],L((o,) for o in its) return detuplify(b[:self.n_inp]),detuplify(b[self.n_inp:]),its def show_batch(self, b=None, max_n=9, ctxs=None, show=True, unique=False, **kwargs): if unique: old_get_idxs = self.get_idxs self.get_idxs = lambda: Inf.zeros if b is None: b = self.one_batch() if not show: return self._pre_show_batch(b, max_n=max_n) show_batch(*self._pre_show_batch(b, max_n=max_n), ctxs=ctxs, max_n=max_n, **kwargs) if unique: self.get_idxs = old_get_idxs def show_results(self, b, out, max_n=9, ctxs=None, show=True, **kwargs): x,y,its = self.show_batch(b, max_n=max_n, show=False) b_out = type(b)(b[:self.n_inp] + (tuple(out) if is_listy(out) else (out,))) x1,y1,outs = self.show_batch(b_out, max_n=max_n, show=False) res = (x,x1,None,None) if its is None else (x, y, its, outs.itemgot(slice(self.n_inp,None))) if not show: return res show_results(*res, ctxs=ctxs, max_n=max_n, **kwargs) @property def n_inp(self): if hasattr(self.dataset, &#39;n_inp&#39;): return self.dataset.n_inp if not hasattr(self, &#39;_n_inp&#39;): self._one_pass() return self._n_inp def to(self, device): self.device = device for tfm in self.after_batch.fs: for a in L(getattr(tfm, &#39;parameters&#39;, None)): setattr(tfm, a, getattr(tfm, a).to(device)) return self . A TfmdDL is a DataLoader that creates Pipeline from a list of Transforms for the callbacks after_item, before_batch and after_batch. As a result, it can decode or show a processed batch. . (TfmdDL, decode=&quot;Decode b using tfms&quot;, decode_batch=&quot;Decode b entirely&quot;, new=&quot;Create a new version of self with a few changed attributes&quot;, show_batch=&quot;Show b (defaults to one_batch), a list of lists of pipeline outputs (i.e. output of a DataLoader)&quot;, show_results=&quot;Show each item of b and out&quot;, before_iter=&quot;override&quot;, to=&quot;Put self and its transforms state on device&quot;) . class _Category(int, ShowTitle): pass . aa = torch.randn(1,5) aa . tensor([[-1.2118, 0.8821, 0.2013, -0.3173, 0.5078]]) . TensorImage?? . #Test retain type class NegTfm(Transform): def encodes(self, x): return torch.neg(x) def decodes(self, x): return torch.neg(x) tdl = TfmdDL([(TensorImage([1]),)] * 16, after_batch=NegTfm(), bs=4, num_workers=4) b = tdl.one_batch() b test_eq(type(b[0]), TensorImage) b = (tensor([1.,1.,1.,1.]),) test_eq(type(tdl.decode_batch(b)[0][0]), TensorImage) . (TensorImage([[-1], [-1], [-1], [-1]]),) . bb = NegTfm()(aa) bb . tensor([[ 1.2118, -0.8821, -0.2013, 0.3173, -0.5078]]) . NegTfm().decodes(bb) . tensor([[-1.2118, 0.8821, 0.2013, -0.3173, 0.5078]]) . class A(Transform): def encodes(self, x): return x def decodes(self, x): return TitledInt(x) @Transform def f(x)-&gt;None: return fastuple((x,x)) start = torch.arange(50) test_eq_type(f(2), fastuple((2,2))) . start . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . a = A() tdl = TfmdDL(start, after_item=lambda x: (a(x), f(x)), bs=4) x,y = tdl.one_batch() x y test_eq(type(y), fastuple) s = tdl.decode_batch((x,y)) s test_eq(type(s[0][1]), fastuple) . tensor([0, 1, 2, 3]) . (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) . (#4) [(tensor(0), (tensor(0), tensor(0))),(tensor(1), (tensor(1), tensor(1))),(tensor(2), (tensor(2), tensor(2))),(tensor(3), (tensor(3), tensor(3)))] . it = iter(tdl) . next(it) . (tensor([0, 1, 2, 3]), (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))) . next(it) . (tensor([4, 5, 6, 7]), (tensor([4, 5, 6, 7]), tensor([4, 5, 6, 7]))) . tdl = TfmdDL([1,2,3,4,5], after_item=lambda o : o*2, after_batch=lambda i: -i, bs=4,drop_last=True) tdl L(tdl) # tdl.show_batch() # 此处无法使用show_batch, 因为无法对tensor使用这个功能,不过下面的代码中,经过A函数编码之后,输出了一个list,可以进行show功能 . &lt;__main__.TfmdDL at 0x7fda20b2e670&gt; . (#1) [tensor([-2, -4, -6, -8])] . aa = [1,2,3] bb = L(1,2,3) test_eq(aa,bb) . temp = A() temp(aa) type(temp(aa)) . [1, 2, 3] . list . tdl = TfmdDL(torch.arange(0,50), after_item=A(), bs=4) tdl.dataset tdl.dataset[0] L(tdl) tdl.show_batch() test_eq(tdl.dataset[0], start[0]) test_eq(len(tdl), (50-1)//4+1) test_eq(tdl.bs, 4) test_stdout(tdl.show_batch, &#39;0 n1 n2 n3&#39;) test_stdout(partial(tdl.show_batch, unique=True), &#39;0 n0 n0 n0&#39;) . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . tensor(0) . (#13) [tensor([0, 1, 2, 3]),tensor([4, 5, 6, 7]),tensor([ 8, 9, 10, 11]),tensor([12, 13, 14, 15]),tensor([16, 17, 18, 19]),tensor([20, 21, 22, 23]),tensor([24, 25, 26, 27]),tensor([28, 29, 30, 31]),tensor([32, 33, 34, 35]),tensor([36, 37, 38, 39])...] . 0 1 2 3 . tdl tdl.dataset len(tdl) L(tdl) tdl.show_batch() . &lt;__main__.TfmdDL at 0x7fda2052fd90&gt; . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . 13 . (#13) [tensor([ 0, -1, -2, -3]),tensor([-4, -5, -6, -7]),tensor([ -8, -9, -10, -11]),tensor([-12, -13, -14, -15]),tensor([-16, -17, -18, -19]),tensor([-20, -21, -22, -23]),tensor([-24, -25, -26, -27]),tensor([-28, -29, -30, -31]),tensor([-32, -33, -34, -35]),tensor([-36, -37, -38, -39])...] . 0 1 2 3 . default_device() . device(type=&#39;cuda&#39;, index=0) . NegTfm? . torch.cuda.set_device(1) . default_device() . device(type=&#39;cuda&#39;, index=1) . class B(Transform): parameters = &#39;a&#39; def __init__(self): self.a = torch.tensor(0.) def encodes(self, x): x tdl = TfmdDL([(TensorImage([1]),)] * 4, after_batch=[B(),NegTfm()], bs=4) tdl tdl.after_batch # fs means functions tdl.after_batch.fs tdl.after_batch.fs[0].a.device test_eq(tdl.after_batch.fs[0].a.device, torch.device(&#39;cpu&#39;)) tdl.to(default_device()) test_eq(tdl.after_batch.fs[0].a.device, default_device()) . &lt;__main__.TfmdDL at 0x7fda21023640&gt; . Pipeline: B -&gt; NegTfm . (#2) [B: encodes: (object,object) -&gt; encodes decodes: ,NegTfm: encodes: (object,object) -&gt; encodes decodes: (object,object) -&gt; decodes ] . device(type=&#39;cpu&#39;) . &lt;__main__.TfmdDL at 0x7fda21023640&gt; . Methods . (TfmdDL.one_batch) . &lt;function fastai.data.load.DataLoader.one_batch(self)&gt; . tfm = NegTfm() tdl = TfmdDL(start, after_batch=tfm, bs=4) . b = tdl.one_batch() test_eq(tensor([0,-1,-2,-3]), b) . (TfmdDL.decode) . &lt;function __main__.TfmdDL.decode(self, b)&gt; . test_eq(tdl.decode(b), tensor(0,1,2,3)) . (TfmdDL.decode_batch) . &lt;function __main__.TfmdDL.decode_batch(self, b, max_n=9, full=True)&gt; . test_eq(tdl.decode_batch(b), [0,1,2,3]) . (TfmdDL.show_batch) . &lt;function __main__.TfmdDL.show_batch(self, b=None, max_n=9, ctxs=None, show=True, unique=False, **kwargs)&gt; . (TfmdDL.to) . &lt;function __main__.TfmdDL.to(self, device)&gt; . 此处的dataloaders不同于上一章节中的dataloader . class DataLoaders(GetAttr): &quot;Basic wrapper around several `DataLoader`s.&quot; _default=&#39;train&#39; def __init__(self, *loaders, path=&#39;.&#39;, device=None): self.loaders,self.path = list(loaders),Path(path) if device is not None or hasattr(loaders[0],&#39;to&#39;): self.device = device def __getitem__(self, i): return self.loaders[i] def new_empty(self): loaders = [dl.new(dl.dataset.new_empty()) for dl in self.loaders] return type(self)(*loaders, path=self.path, device=self.device) def _set(i, self, v): self.loaders[i] = v train ,valid = add_props(lambda i,x: x[i], _set) train_ds,valid_ds = add_props(lambda i,x: x[i].dataset) @property def device(self): return self._device @device.setter def device(self, d): for dl in self.loaders: dl.to(d) self._device = d def to(self, device): self.device = device return self def cuda(self): return self.to(device=default_device()) def cpu(self): return self.to(device=torch.device(&#39;cpu&#39;)) @classmethod def from_dsets(cls, *ds, path=&#39;.&#39;, bs=64, device=None, dl_type=TfmdDL, **kwargs): default = (True,) + (False,) * (len(ds)-1) defaults = {&#39;shuffle&#39;: default, &#39;drop_last&#39;: default} for nm in _batch_tfms: if nm in kwargs: kwargs[nm] = Pipeline(kwargs[nm]) kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()}) kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)] return cls(*[dl_type(d, bs=bs, **k) for d,k in zip(ds, kwargs)], path=path, device=device) @classmethod def from_dblock(cls, dblock, source, path=&#39;.&#39;, bs=64, val_bs=None, shuffle_train=True, device=None, **kwargs): return dblock.dataloaders(source, path=path, bs=bs, val_bs=val_bs, shuffle_train=shuffle_train, device=device, **kwargs) _docs=dict(__getitem__=&quot;Retrieve `DataLoader` at `i` (`0` is training, `1` is validation)&quot;, train=&quot;Training `DataLoader`&quot;, valid=&quot;Validation `DataLoader`&quot;, train_ds=&quot;Training `Dataset`&quot;, valid_ds=&quot;Validation `Dataset`&quot;, to=&quot;Use `device`&quot;, cuda=&quot;Use the gpu if available&quot;, cpu=&quot;Use the cpu&quot;, new_empty=&quot;Create a new empty version of `self` with the same transforms&quot;, from_dblock=&quot;Create a dataloaders from a given `dblock`&quot;) . tdl.one_batch() . tensor([ 0, -1, -2, -3]) . dls = DataLoaders(tdl,tdl) dls.train x = dls.train.one_batch() x x2 = first(tdl) test_eq(x,x2) x2 = dls.one_batch() x2 test_eq(x,x2) . &lt;__main__.TfmdDL at 0x7fda0dfd81f0&gt; . tensor([ 0, -1, -2, -3]) . tensor([ 0, -1, -2, -3]) . #test assignment works dls.train = dls.train.new(bs=4) . dls.train . &lt;__main__.TfmdDL at 0x7fda212a89d0&gt; . dls.train_ds . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . dls.valid_ds . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) . Methods . (DataLoaders.__getitem__) . DataLoaders.__getitem__[source] . DataLoaders.__getitem__(i) . Retrieve DataLoader at i (0 is training, 1 is validation) . dls[0] dls[1] . &lt;__main__.TfmdDL at 0x7fda212a89d0&gt; . &lt;__main__.TfmdDL at 0x7fda0dfd81f0&gt; . x2 = dls[0].one_batch() test_eq(x,x2) . class FilteredBase: &quot;Base class for lists with subsets&quot; _dl_type,_dbunch_type = TfmdDL,DataLoaders def __init__(self, *args, dl_type=None, **kwargs): if dl_type is not None: self._dl_type = dl_type self.dataloaders = delegates(self._dl_type.__init__)(self.dataloaders) super().__init__(*args, **kwargs) @property def n_subsets(self): return len(self.splits) def _new(self, items, **kwargs): return super()._new(items, splits=self.splits, **kwargs) def subset(self): raise NotImplemented def dataloaders(self, bs=64, val_bs=None, shuffle_train=True, n=None, path=&#39;.&#39;, dl_type=None, dl_kwargs=None, device=None, **kwargs): if device is None: device=default_device() if dl_kwargs is None: dl_kwargs = [{}] * self.n_subsets if dl_type is None: dl_type = self._dl_type drop_last = kwargs.pop(&#39;drop_last&#39;, shuffle_train) dl = dl_type(self.subset(0), bs=bs, shuffle=shuffle_train, drop_last=drop_last, n=n, device=device, **merge(kwargs, dl_kwargs[0])) dls = [dl] + [dl.new(self.subset(i), bs=(bs if val_bs is None else val_bs), shuffle=False, drop_last=False, n=None, **dl_kwargs[i]) for i in range(1, self.n_subsets)] return self._dbunch_type(*dls, path=path, device=device) FilteredBase.train,FilteredBase.valid = add_props(lambda i,x: x.subset(i)) . class TfmdLists(FilteredBase, L, GetAttr): &quot;A `Pipeline` of `tfms` applied to a collection of `items`&quot; _default=&#39;tfms&#39; def __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True, splits=None, types=None, verbose=False, dl_type=None): super().__init__(items, use_list=use_list) if dl_type is not None: self._dl_type = dl_type self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs) if isinstance(tfms,TfmdLists): tfms = tfms.tfms if isinstance(tfms,Pipeline): do_setup=False self.tfms = Pipeline(tfms, split_idx=split_idx) store_attr(&#39;types,split_idx&#39;) if do_setup: pv(f&quot;Setting up {self.tfms}&quot;, verbose) self.setup(train_setup=train_setup) def _new(self, items, split_idx=None, **kwargs): split_idx = ifnone(split_idx,self.split_idx) return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, split_idx=split_idx, **kwargs) def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i) def _after_item(self, o): return self.tfms(o) def __repr__(self): return f&quot;{self.__class__.__name__}: {self.items} ntfms - {self.tfms.fs}&quot; def __iter__(self): return (self[i] for i in range(len(self))) def show(self, o, **kwargs): return self.tfms.show(o, **kwargs) def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs) def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs) def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1)) def new_empty(self): return self._new([]) def setup(self, train_setup=True): self.tfms.setup(self, train_setup) if len(self) != 0: x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0] self.types = [] for f in self.tfms.fs: self.types.append(getattr(f, &#39;input_types&#39;, type(x))) x = f(x) self.types.append(type(x)) types = L(t if is_listy(t) else [t] for t in self.types).concat().unique() self.pretty_types = &#39; n&#39;.join([f&#39; - {t}&#39; for t in types]) def infer_idx(self, x): # TODO: check if we really need this, or can simplify idx = 0 for t in self.types: if isinstance(x, t): break idx += 1 types = L(t if is_listy(t) else [t] for t in self.types).concat().unique() pretty_types = &#39; n&#39;.join([f&#39; - {t}&#39; for t in types]) assert idx &lt; len(self.types), f&quot;Expected an input of type in n{pretty_types} n but got {type(x)}&quot; return idx def infer(self, x): return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx) def __getitem__(self, idx): res = super().__getitem__(idx) if self._after_item is None: return res return self._after_item(res) if is_indexer(idx) else res.map(self._after_item) . (TfmdLists, setup=&quot;Transform setup with self&quot;, decode=&quot;From Pipeline&quot;, show=&quot;From Pipeline&quot;, overlapping_splits=&quot;All splits that are in more than one split&quot;, subset=&quot;New TfmdLists with same tfms that only includes items in ith split&quot;, infer_idx=&quot;Finds the index where self.tfms can be applied to x, depending on the type of x&quot;, infer=&quot;Apply self.tfms to x starting at the right tfm depending on the type of x&quot;, new_empty=&quot;A new version of self but with no items&quot;) . def decode_at(o, idx): &quot;Decoded item at `idx`&quot; return o.decode(o[idx]) . def show_at(o, idx, **kwargs): &quot;Show item at `idx`&quot;, return o.show(o[idx], **kwargs) . A TfmdLists combines a collection of object with a Pipeline. tfms can either be a Pipeline or a list of transforms, in which case, it will wrap them in a Pipeline. use_list is passed along to L with the items and split_idx are passed to each transform of the Pipeline. do_setup indicates if the Pipeline.setup method should be called during initialization. . TitledInt?? . class _IntFloatTfm(Transform): def encodes(self, o): return TitledInt(o) def decodes(self, o): return TitledFloat(o) int2f_tfm=_IntFloatTfm() def _neg(o): return -o neg_tfm = Transform(_neg, _neg) . aa = neg_tfm(2) aa . -2 . items = L([1.,2.,3.]); tfms = [neg_tfm, int2f_tfm] tl = TfmdLists(items, tfms=tfms) test_eq_type(tl[0], TitledInt(-1)) test_eq_type(tl[1], TitledInt(-2)) test_eq_type(tl.decode(tl[2]), TitledFloat(3.)) test_stdout(lambda: show_at(tl, 2), &#39;-3&#39;) test_eq(tl.types, [float, float, TitledInt]) tl . TfmdLists: [1.0, 2.0, 3.0] tfms - [_neg: encodes: (object,object) -&gt; _negdecodes: (object,object) -&gt; _neg, _IntFloatTfm: encodes: (object,object) -&gt; encodes decodes: (object,object) -&gt; decodes ] . # add splits to TfmdLists splits = [[0,2],[1]] tl = TfmdLists(items, tfms=tfms, splits=splits) test_eq(tl.n_subsets, 2) test_eq(tl.train, tl.subset(0)) test_eq(tl.valid, tl.subset(1)) test_eq(tl.train.items, items[splits[0]]) test_eq(tl.valid.items, items[splits[1]]) test_eq(tl.train.tfms.split_idx, 0) test_eq(tl.valid.tfms.split_idx, 1) test_eq(tl.train.new_empty().split_idx, 0) test_eq(tl.valid.new_empty().split_idx, 1) test_eq_type(tl.splits, L(splits)) assert not tl.overlapping_splits() . df = pd.DataFrame(dict(a=[1,2,3],b=[2,3,4])) tl = TfmdLists(df, lambda o: o.a+1, splits=[[0],[1,2]]) test_eq(tl[1,2], [3,4]) tr = tl.subset(0) test_eq(tr[:], [2]) val = tl.subset(1) test_eq(val[:], [3,4]) . items . (#3) [1.0,2.0,3.0] . class _B(Transform): def __init__(self): self.m = 0 def encodes(self, o): return o+self.m def decodes(self, o): return o-self.m def setups(self, items): print(items) self.m = tensor(items).float().mean().item() # test for setup, which updates `self.m` tl = TfmdLists(items, _B()) test_eq(tl.m, 2) . TfmdLists: [1.0, 2.0, 3.0] tfms - [] . Here&#39;s how we can use TfmdLists.setup to implement a simple category list, getting labels from a mock file list: . class _Cat(Transform): order = 1 def encodes(self, o): return int(self.o2i[o]) def decodes(self, o): return TitledStr(self.vocab[o]) def setups(self, items): self.vocab,self.o2i = uniqueify(L(items), sort=True, bidir=True) tcat = _Cat() def _lbl(o): return TitledStr(o.split(&#39;_&#39;)[0]) # Check that tfms are sorted by `order` &amp; `_lbl` is called first fns = [&#39;dog_0.jpg&#39;,&#39;cat_0.jpg&#39;,&#39;cat_2.jpg&#39;,&#39;cat_1.jpg&#39;,&#39;dog_1.jpg&#39;] tl = TfmdLists(fns, [tcat,_lbl]) exp_voc = [&#39;cat&#39;,&#39;dog&#39;] test_eq(tcat.vocab, exp_voc) test_eq(tl.tfms.vocab, exp_voc) test_eq(tl.vocab, exp_voc) test_eq(tl, (1,0,0,0,1)) test_eq([tl.decode(o) for o in tl], (&#39;dog&#39;,&#39;cat&#39;,&#39;cat&#39;,&#39;cat&#39;,&#39;dog&#39;)) . #Check only the training set is taken into account for setup tl = TfmdLists(fns, [tcat,_lbl], splits=[[0,4], [1,2,3]]) test_eq(tcat.vocab, [&#39;dog&#39;]) . tfm = NegTfm(split_idx=1) tds = TfmdLists(start, A()) tdl = TfmdDL(tds, after_batch=tfm, bs=4) x = tdl.one_batch() test_eq(x, torch.arange(4)) tds.split_idx = 1 x = tdl.one_batch() test_eq(x, -torch.arange(4)) tds.split_idx = 0 x = tdl.one_batch() test_eq(x, torch.arange(4)) . tds = TfmdLists(start, A()) tdl = TfmdDL(tds, after_batch=NegTfm(), bs=4) test_eq(tdl.dataset[0], start[0]) test_eq(len(tdl), (len(tds)-1)//4+1) test_eq(tdl.bs, 4) test_stdout(tdl.show_batch, &#39;0 n1 n2 n3&#39;) . (TfmdLists.subset) . &lt;function __main__.TfmdLists.subset(self, i)&gt; . (TfmdLists.infer_idx) . &lt;function __main__.TfmdLists.infer_idx(self, x)&gt; . (TfmdLists.infer) . &lt;function __main__.TfmdLists.infer(self, x)&gt; . def mult(x): return x*2 mult.order = 2 fns = [&#39;dog_0.jpg&#39;,&#39;cat_0.jpg&#39;,&#39;cat_2.jpg&#39;,&#39;cat_1.jpg&#39;,&#39;dog_1.jpg&#39;] tl = TfmdLists(fns, [_lbl,_Cat(),mult]) test_eq(tl.infer_idx(&#39;dog_45.jpg&#39;), 0) test_eq(tl.infer(&#39;dog_45.jpg&#39;), 2) test_eq(tl.infer_idx(4), 2) test_eq(tl.infer(4), 8) test_fail(lambda: tl.infer_idx(2.0)) test_fail(lambda: tl.infer(2.0)) . #Test input_types works on a Transform cat = _Cat() cat.input_types = (str, float) tl = TfmdLists(fns, [_lbl,cat,mult]) test_eq(tl.infer_idx(2.0), 1) #Test type annotations work on a function def mult(x:(int,float)): return x*2 mult.order = 2 tl = TfmdLists(fns, [_lbl,_Cat(),mult]) test_eq(tl.infer_idx(2.0), 2) . @delegates(TfmdLists) class Datasets(FilteredBase): &quot;A dataset that creates a tuple from each `tfms`, passed through `item_tfms`&quot; def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs): super().__init__(dl_type=dl_type) self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))]) self.n_inp = ifnone(n_inp, max(1, len(self.tls)-1)) def __getitem__(self, it): res = tuple([tl[it] for tl in self.tls]) return res if is_indexer(it) else list(zip(*res)) def __getattr__(self,k): return gather_attrs(self, k, &#39;tls&#39;) def __dir__(self): return super().__dir__() + gather_attr_names(self, &#39;tls&#39;) def __len__(self): return len(self.tls[0]) def __iter__(self): return (self[i] for i in range(len(self))) def __repr__(self): return coll_repr(self) def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o))) def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp) def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs) def overlapping_splits(self): return self.tls[0].overlapping_splits() def new_empty(self): return type(self)(tls=[tl.new_empty() for tl in self.tls], n_inp=self.n_inp) @property def splits(self): return self.tls[0].splits @property def split_idx(self): return self.tls[0].tfms.split_idx @property def items(self): return self.tls[0].items @items.setter def items(self, v): for tl in self.tls: tl.items = v def show(self, o, ctx=None, **kwargs): for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs) return ctx @contextmanager def set_split_idx(self, i): old_split_idx = self.split_idx for tl in self.tls: tl.tfms.split_idx = i try: yield self finally: for tl in self.tls: tl.tfms.split_idx = old_split_idx _docs=dict( decode=&quot;Compose `decode` of all `tuple_tfms` then all `tfms` on `i`&quot;, show=&quot;Show item `o` in `ctx`&quot;, dataloaders=&quot;Get a `DataLoaders`&quot;, overlapping_splits=&quot;All splits that are in more than one split&quot;, subset=&quot;New `Datasets` that only includes subset `i`&quot;, new_empty=&quot;Create a new empty version of the `self`, keeping only the transforms&quot;, set_split_idx=&quot;Contextmanager to use the same `Datasets` with another `split_idx`&quot; ) . A Datasets creates a tuple from items (typically input,target) by applying to them each list of Transform (or Pipeline) in tfms. Note that if tfms contains only one list of tfms, the items given by Datasets will be tuples of one element. . n_inp is the number of elements in the tuples that should be considered part of the input and will default to 1 if tfms consists of one set of transforms, len(tfms)-1 otherwise. In most cases, the number of elements in the tuples spit out by Datasets will be 2 (for input,target) but it can happen that there is 3 (Siamese networks or tabular data) in which case we need to be able to determine when the inputs end and the targets begin. . add(2) . &lt;function fastcore.basics._oper.&lt;locals&gt;.&lt;lambda&gt;(o)&gt; . items =[1,2,3,4] # 下面定义了两组变换形式, 求负数与类型转换+加一的操作 dsets = Datasets(items, [[neg_tfm,int2f_tfm], [add(1)]]) dsets t = dsets[0] test_eq(t, (-1,2)) test_eq(dsets[0,1,2], [(-1,2),(-2,3),(-3,4)]) test_eq(dsets.n_inp, 1) dsets.decode(t) . (#4) [(-1, 2),(-2, 3),(-3, 4),(-4, 5)] . (1.0, 2) . Norm&#21464;&#25442;example . class Norm(Transform): def encodes(self, o): return (o-self.m)/self.s def decodes(self, o): return (o*self.s)+self.m def setups(self, items): its = tensor(items).float() self.m,self.s = its.mean(),its.std() . items = [1,2,3,4] nrm = Norm() dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm,nrm]]) dsets x,y = zip(*dsets) x y # 实行变换后,可以直接从子类中取出mean这个属性值 nrm.m test_close(tensor(y).mean(), 0) test_close(tensor(y).std(), 1) test_eq(x, (-1,-2,-3,-4,)) test_eq(nrm.m, -2.5) test_stdout(lambda:show_at(dsets, 1), &#39;-2&#39;) test_eq(dsets.m, nrm.m) test_eq(dsets.norm.m, nrm.m) test_eq(dsets.train.norm.m, nrm.m) . (#4) [(-1, tensor(1.1619)),(-2, tensor(0.3873)),(-3, tensor(-0.3873)),(-4, tensor(-1.1619))] . (-1, -2, -3, -4) . (tensor(1.1619), tensor(0.3873), tensor(-0.3873), tensor(-1.1619)) . tensor(-2.5000) . #Check filtering is properly applied class B(Transform): def encodes(self, x)-&gt;None: return int(x+1) def decodes(self, x): return TitledInt(x-1) add1 = B(split_idx=1) dsets = Datasets(items, [neg_tfm, [neg_tfm,int2f_tfm,add1]], splits=[[3],[0,1,2]]) test_eq(dsets[1], [-2,-2]) test_eq(dsets.valid[1], [-2,-1]) test_eq(dsets.valid[[1,1]], [[-2,-1], [-2,-1]]) test_eq(dsets.train[0], [-4,-4]) . _Cat() . _Cat: encodes: (object,object) -&gt; encodes decodes: (object,object) -&gt; decodes . test_fns = [&#39;dog_0.jpg&#39;,&#39;cat_0.jpg&#39;,&#39;cat_2.jpg&#39;,&#39;cat_1.jpg&#39;,&#39;kid_1.jpg&#39;] tcat = _Cat() dsets = Datasets(test_fns, [[tcat,_lbl]], splits=[[0,1,2], [3,4]]) dsets.train dsets.valid[0] test_eq(tcat.vocab, [&#39;cat&#39;,&#39;dog&#39;]) test_eq(dsets.train, [(1,),(0,),(0,)]) test_eq(dsets.valid[0], (0,)) test_stdout(lambda: show_at(dsets.train, 0), &quot;dog&quot;) . (#3) [(1,),(0,),(0,)] . (0,) . inp = [0,1,2,3,4] dsets = Datasets(inp, tfms=[None]) dsets dsets[2] test_eq(*dsets[2], 2) # Retrieve one item (subset 0 is the default) test_eq(dsets[1,2], [(1,),(2,)]) # Retrieve two items by index mask = [True,False,False,True,False] test_eq(dsets[mask], [(0,),(3,)]) # Retrieve two items by mask . (#5) [(0,),(1,),(2,),(3,),(4,)] . (2,) . inp = pd.DataFrame(dict(a=[5,1,2,3,4])) dsets = Datasets(inp, tfms=attrgetter(&#39;a&#39;)).subset(0) inp dsets test_eq(*dsets[2], 2) # Retrieve one item (subset 0 is the default) test_eq(dsets[1,2], [(1,),(2,)]) # Retrieve two items by index mask = [True,False,False,True,False] test_eq(dsets[mask], [(5,),(3,)]) # Retrieve two items by mask . a . 0 5 | . 1 1 | . 2 2 | . 3 3 | . 4 4 | . (#5) [(5,),(1,),(2,),(3,),(4,)] . #test n_inp inp = [0,1,2,3,4] dsets = Datasets(inp) dsets dsets.n_inp dsets = Datasets(inp, tfms=[None]) dsets test_eq(dsets.n_inp, 1) dsets = Datasets(inp, tfms=[[None],[None],[None]]) dsets test_eq(dsets.n_inp, 2) dsets = Datasets(inp, tfms=[[None],[None]]) dsets dsets.n_inp dsets = Datasets(inp, tfms=[[None],[None],[None]], n_inp=1) dsets test_eq(dsets.n_inp, 1) . (#5) [(0,),(1,),(2,),(3,),(4,)] . 1 . (#5) [(0,),(1,),(2,),(3,),(4,)] . (#5) [(0, 0, 0),(1, 1, 1),(2, 2, 2),(3, 3, 3),(4, 4, 4)] . (#5) [(0, 0),(1, 1),(2, 2),(3, 3),(4, 4)] . 1 . (#5) [(0, 0, 0),(1, 1, 1),(2, 2, 2),(3, 3, 3),(4, 4, 4)] . # splits can be indices dsets = Datasets(range(5), tfms=[None], splits=[tensor([0,2]), [1,3,4]]) # dsets dsets.train dsets.valid test_eq(dsets.subset(0), [(0,),(2,)]) test_eq(dsets.train, [(0,),(2,)]) # Subset 0 is aliased to `train` test_eq(dsets.subset(1), [(1,),(3,),(4,)]) test_eq(dsets.valid, [(1,),(3,),(4,)]) # Subset 1 is aliased to `valid` test_eq(*dsets.valid[2], 4) #assert &#39;[(1,),(3,),(4,)]&#39; in str(dsets) and &#39;[(0,),(2,)]&#39; in str(dsets) # dsets . (#2) [(0,),(2,)] . (#3) [(1,),(3,),(4,)] . # splits can be boolean masks (they don&#39;t have to cover all items, but must be disjoint) splits = [[False,True,True,False,True], [True,False,False,False,False]] dsets = Datasets(range(5), tfms=[None], splits=splits) test_eq(dsets.train, [(1,),(2,),(4,)]) test_eq(dsets.valid, [(0,)]) . # apply transforms to all items tfm = [[lambda x: x*2,lambda x: x+1]] splits = [[1,2],[0,3,4]] dsets = Datasets(range(5), tfm, splits=splits) dsets test_eq(dsets.train,[(3,),(5,)]) test_eq(dsets.valid,[(1,),(7,),(9,)]) test_eq(dsets.train[False,True], [(5,)]) . (#5) [(1,),(3,),(5,),(7,),(9,)] . . # only transform subset 1 class _Tfm(Transform): split_idx=1 def encodes(self, x): return x*2 def decodes(self, x): return TitledStr(x//2) . aa = DataLoader(range(5),bs = 2,after_item=_Tfm()) L(aa) . (#3) [tensor([0, 1]),tensor([2, 3]),tensor([4])] . dsets = Datasets(range(5), [_Tfm()]) dsets . (#5) [(0,),(1,),(2,),(3,),(4,)] . dsets = Datasets(range(5), [_Tfm()], splits=[[1,2],[0,3,4]]) dsets # 注意此处的dsets没有发生转换, 反而单独拿出来valid有了变化 test_eq(dsets.train,[(1,),(2,)]) test_eq(dsets.valid,[(0,),(6,),(8,)]) test_eq(dsets.train[False,True], [(2,)]) dsets . (#5) [(0,),(1,),(2,),(3,),(4,)] . (#5) [(0,),(1,),(2,),(3,),(4,)] . #A context manager to change the split_idx and apply the validation transform on the training set ds = dsets.train ds with ds.set_split_idx(1): test_eq(ds,[(2,),(4,)]) test_eq(dsets.train,[(1,),(2,)]) . (#2) [(1,),(2,)] . #Test Datasets pickles dsrc1 = pickle.loads(pickle.dumps(dsets)) test_eq(dsets.train, dsrc1.train) test_eq(dsets.valid, dsrc1.valid) . dsets = Datasets(range(5), [_Tfm(),noop], splits=[[1,2],[0,3,4]]) dsets test_eq(dsets.train,[(1,1),(2,2)]) test_eq(dsets.valid,[(0,0),(6,3),(8,4)]) . (#5) [(0, 0),(1, 1),(2, 2),(3, 3),(4, 4)] . start = torch.arange(0,50) tds = Datasets(start, [A()]) tdl = TfmdDL(tds, after_item=NegTfm(), bs=4) b = tdl.one_batch() test_eq(tdl.decode_batch(b), ((0,),(1,),(2,),(3,))) test_stdout(tdl.show_batch, &quot;0 n1 n2 n3&quot;) . # only transform subset 1 class _Tfm(Transform): split_idx=1 def encodes(self, x): return x*2 dsets = Datasets(range(8), splits=[[1,2,5,7],[0,3,4,6]]) dsets dsets = Datasets(range(8),[ None,None, None], splits=[[1,2,5,7],[0,3,4,6]]) dsets dsets.valid dsets.train dsets.n_inp . (#8) [(0,),(1,),(2,),(3,),(4,),(5,),(6,),(7,)] . (#8) [(0, 0, 0),(1, 1, 1),(2, 2, 2),(3, 3, 3),(4, 4, 4),(5, 5, 5),(6, 6, 6),(7, 7, 7)] . (#4) [(0, 0, 0),(3, 3, 3),(4, 4, 4),(6, 6, 6)] . (#4) [(1, 1, 1),(2, 2, 2),(5, 5, 5),(7, 7, 7)] . 2 . # only transform subset 1 class _Tfm(Transform): split_idx=1 def encodes(self, x): return x*2 dsets = Datasets(range(8), [None], splits=[[1,2,5,7],[0,3,4,6]]) dls = dsets.dataloaders(bs=4, after_batch=_Tfm(), shuffle_train=False, device=torch.device(&#39;cpu&#39;)) test_eq(dls.train, [(tensor([1,2,5, 7]),)]) test_eq(dls.valid, [(tensor([0,6,8,12]),)]) test_eq(dls.n_inp, 1) . Methods . items = [1,2,3,4] dsets = Datasets(items, [[neg_tfm,int2f_tfm]]) . _dsrc = Datasets([1,2]) (_dsrc.dataloaders, name=&quot;Datasets.dataloaders&quot;) . Datasets.dataloaders[source] . Datasets.dataloaders(bs=64, val_bs=None, shuffle_train=True, n=None, path=&#39;.&#39;, dl_type=None, dl_kwargs=None, device=None, shuffle=False, num_workers=None, verbose=False, do_setup=True, pin_memory=False, timeout=0, batch_size=None, drop_last=False, indexed=None, persistent_workers=False, wif=None, before_iter=None, after_item=None, before_batch=None, after_batch=None, after_iter=None, create_batches=None, create_item=None, create_batch=None, retain=None, get_idxs=None, sample=None, shuffle_fn=None, do_batch=None) . Get a DataLoaders . (Datasets.decode) . Datasets.decode[source] . Datasets.decode(o, full=True) . Compose decode of all tuple_tfms then all tfms on i . test_eq(*dsets[0], -1) test_eq(*dsets.decode((-1,)), 1) . (Datasets.show) . Datasets.show[source] . Datasets.show(o, ctx=None, **kwargs) . Show item o in ctx . test_stdout(lambda:dsets.show(dsets[1]), &#39;-2&#39;) . (Datasets.new_empty) . Datasets.new_empty[source] . Datasets.new_empty() . Create a new empty version of the self, keeping only the transforms . items = [1,2,3,4] nrm = Norm() dsets = Datasets(items, [[neg_tfm,int2f_tfm], [neg_tfm]]) empty = dsets.new_empty() test_eq(empty.items, []) . #test it works for dataframes too df = pd.DataFrame({&#39;a&#39;:[1,2,3,4,5], &#39;b&#39;:[6,7,8,9,10]}) dsets = Datasets(df, [[attrgetter(&#39;a&#39;)], [attrgetter(&#39;b&#39;)]]) empty = dsets.new_empty() . Add test set for inference . # only transform subset 1 class _Tfm1(Transform): split_idx=0 def encodes(self, x): return x*3 dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]]) test_eq(dsets.train, [(3,),(6,),(15,),(21,)]) test_eq(dsets.valid, [(0,),(6,),(8,),(12,)]) . def test_set(dsets, test_items, rm_tfms=None, with_labels=False): &quot;Create a test set from `test_items` using validation transforms of `dsets`&quot; if isinstance(dsets, Datasets): tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp] test_tls = [tl._new(test_items, split_idx=1) for tl in tls] if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls] else: rm_tfms = tuplify(rm_tfms, match=test_tls) for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:] return Datasets(tls=test_tls) elif isinstance(dsets, TfmdLists): test_tl = dsets._new(test_items, split_idx=1) if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items)) test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:] return test_tl else: raise Exception(f&quot;This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}&quot;) . class _Tfm1(Transform): split_idx=0 def encodes(self, x): return x*3 dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]]) test_eq(dsets.train, [(3,),(6,),(15,),(21,)]) test_eq(dsets.valid, [(0,),(6,),(8,),(12,)]) #Tranform of the validation set are applied tst = test_set(dsets, [1,2,3]) test_eq(tst, [(2,),(4,),(6,)]) . #Test with different types tfm = _Tfm1() tfm.split_idx,tfm.order = None,2 dsets = Datasets([&#39;dog&#39;, &#39;cat&#39;, &#39;cat&#39;, &#39;dog&#39;], [[_Cat(),tfm]]) #With strings test_eq(test_set(dsets, [&#39;dog&#39;, &#39;cat&#39;, &#39;cat&#39;]), [(3,), (0,), (0,)]) #With ints test_eq(test_set(dsets, [1,2]), [(3,), (6,)]) . #Test with various input lengths dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]]) tst = test_set(dsets, [1,2,3]) test_eq(tst, [(2,2),(4,4),(6,6)]) dsets = Datasets(range(8), [[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()],[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=1) tst = test_set(dsets, [1,2,3]) test_eq(tst, [(2,),(4,),(6,)]) . #Test with rm_tfms dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]]) tst = test_set(dsets, [1,2,3]) test_eq(tst, [(4,),(8,),(12,)]) dsets = Datasets(range(8), [[_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]]) tst = test_set(dsets, [1,2,3], rm_tfms=1) test_eq(tst, [(2,),(4,),(6,)]) dsets = Datasets(range(8), [[_Tfm(),_Tfm()], [_Tfm(),_Tfm()]], splits=[[1,2,5,7],[0,3,4,6]], n_inp=2) tst = test_set(dsets, [1,2,3], rm_tfms=(1,0)) test_eq(tst, [(2,4),(4,8),(6,12)]) . @patch @delegates(TfmdDL.__init__) def test_dl(self:DataLoaders, test_items, rm_type_tfms=None, with_labels=False, **kwargs): &quot;Create a test dataloader from `test_items` using validation transforms of `dls`&quot; test_ds = test_set(self.valid_ds, test_items, rm_tfms=rm_type_tfms, with_labels=with_labels ) if isinstance(self.valid_ds, (Datasets, TfmdLists)) else test_items return self.valid.new(test_ds, **kwargs) . dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]]) dls = dsets.dataloaders(bs=4, device=torch.device(&#39;cpu&#39;)) . dsets = Datasets(range(8), [[_Tfm(),_Tfm1()]], splits=[[1,2,5,7],[0,3,4,6]]) dls = dsets.dataloaders(bs=4, device=torch.device(&#39;cpu&#39;)) tst_dl = dls.test_dl([2,3,4,5]) test_eq(tst_dl._n_inp, 1) test_eq(list(tst_dl), [(tensor([ 4, 6, 8, 10]),)]) #Test you can change transforms tst_dl = dls.test_dl([2,3,4,5], after_item=add1) test_eq(list(tst_dl), [(tensor([ 5, 7, 9, 11]),)]) .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/31/fastai-datacore.html",
            "relUrl": "/pytorch/fastai/2021/01/31/fastai-datacore.html",
            "date": " • Jan 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "fastai dataloader",
            "content": "from fastai.torch_basics import * from torch.utils.data.dataloader import _MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter,_DatasetKind _loaders = (_MultiProcessingDataLoaderIter,_SingleProcessDataLoaderIter) . # from nbdev.showdoc import * . bs = 4 letters = list(string.ascii_lowercase) . DataLoader helpers . fastai includes a replacement for Pytorch&#39;s DataLoader which is largely API-compatible, and adds a lot of useful functionality and flexibility. Before we look at the class, there are a couple of helpers we&#39;ll need to define. . def _wif(worker_id): set_num_threads(1) info = get_worker_info() ds = info.dataset.d ds.num_workers,ds.offs = info.num_workers,info.id set_seed(info.seed) ds.wif() class _FakeLoader: _IterableDataset_len_called,_auto_collation,collate_fn,drop_last = None,False,noops,False _index_sampler,generator,prefetch_factor = Inf.count,None,2 dataset_kind = _dataset_kind = _DatasetKind.Iterable def __init__(self, d, pin_memory, num_workers, timeout, persistent_workers): self.dataset,self.default,self.worker_init_fn = self,d,_wif store_attr(&#39;d,pin_memory,num_workers,timeout,persistent_workers&#39;) def __iter__(self): return iter(self.d.create_batches(self.d.sample())) @property def multiprocessing_context(self): return (None,multiprocessing)[self.num_workers&gt;0] @contextmanager def no_multiproc(self): old_num_workers = self.num_workers try: self.num_workers = 0 yield self.d finally: self.num_workers = old_num_workers _collate_types = (ndarray, Tensor, typing.Mapping, str) . def fa_collate(t): &quot;A replacement for PyTorch `default_collate` which maintains types and handles `Sequence`s&quot; b = t[0] return (default_collate(t) if isinstance(b, _collate_types) else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) else default_collate(t)) . #e.g. x is int, y is tuple t = [(1,(2,3)),(1,(2,3))] test_eq(fa_collate(t), default_collate(t)) test_eq(L(fa_collate(t)).map(type), [Tensor,tuple]) . t fa_collate(t) . [(1, (2, 3)), (1, (2, 3))] . (tensor([1, 1]), (tensor([2, 2]), tensor([3, 3]))) . t = [(1,(2,(3,4))),(1,(2,(3,4)))] test_eq(fa_collate(t), default_collate(t)) test_eq(L(fa_collate(t)).map(type), [Tensor,tuple]) test_eq(L(fa_collate(t)[1]).map(type), [Tensor,tuple]) . t fa_collate(t) fa_collate(t)[1] len(fa_collate(t)) . [(1, (2, 3)), (1, (2, 3))] . (tensor([1, 1]), (tensor([2, 2]), tensor([3, 3]))) . (tensor([2, 2]), tensor([3, 3])) . 2 . assemble data into dataset with pytorch . https://zhuanlan.zhihu.com/p/30385675 . default_collate?? . t fa_collate(t) . [(1, (2, (3, 4))), (1, (2, (3, 4)))] . (tensor([1, 1]), (tensor([2, 2]), (tensor([3, 3]), tensor([4, 4])))) . def fa_convert(t): &quot;A replacement for PyTorch `default_convert` which maintains types and handles `Sequence`s&quot; return (default_convert(t) if isinstance(t, _collate_types) else type(t)([fa_convert(s) for s in t]) if isinstance(t, Sequence) else default_convert(t)) . t0 = array([1,2]) t = [t0,(t0,t0)] test_eq(fa_convert(t), default_convert(t)) test_eq(L(fa_convert(t)).map(type), [Tensor,tuple]) . t fa_convert(t) . [array([1, 2]), (array([1, 2]), array([1, 2]))] . [tensor([1, 2]), (tensor([1, 2]), tensor([1, 2]))] . class SkipItemException(Exception): &quot;Raised to notify `DataLoader` to skip an item&quot; pass . @funcs_kwargs class DataLoader(GetAttr): _noop_methods = &#39;wif before_iter after_item before_batch after_batch after_iter&#39;.split() for o in _noop_methods: exec(f&quot;def {o}(self, x=None, *args, **kwargs): return x&quot;) _methods = _noop_methods + &#39;create_batches create_item create_batch retain get_idxs sample shuffle_fn do_batch create_batch&#39;.split() _default = &#39;dataset&#39; def __init__(self, dataset=None, bs=None, num_workers=0, pin_memory=False, timeout=0, batch_size=None, shuffle=False, drop_last=False, indexed=None, n=None, device=None, persistent_workers=False, **kwargs): if batch_size is not None: bs = batch_size # PyTorch compatibility assert not (bs is None and drop_last) if indexed is None: indexed = dataset is not None and hasattr(dataset,&#39;__getitem__&#39;) if n is None: try: n = len(dataset) except TypeError: pass store_attr(&#39;dataset,bs,shuffle,drop_last,indexed,n,pin_memory,timeout,device&#39;) self.rng,self.num_workers,self.offs = random.Random(random.randint(0,2**32-1)),1,0 self.fake_l = _FakeLoader(self, pin_memory, num_workers, timeout, persistent_workers=persistent_workers) def __len__(self): if self.n is None: raise TypeError if self.bs is None: return self.n return self.n//self.bs + (0 if self.drop_last or self.n%self.bs==0 else 1) def get_idxs(self): idxs = Inf.count if self.indexed else Inf.nones if self.n is not None: idxs = list(itertools.islice(idxs, self.n)) if self.shuffle: idxs = self.shuffle_fn(idxs) return idxs def sample(self): return (b for i,b in enumerate(self.__idxs) if i//(self.bs or 1)%self.num_workers==self.offs) def __iter__(self): self.randomize() self.before_iter() self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses) for b in _loaders[self.fake_l.num_workers==0](self.fake_l): if self.device is not None: b = to_device(b, self.device) yield self.after_batch(b) self.after_iter() if hasattr(self, &#39;it&#39;): del(self.it) def create_batches(self, samps): self.it = iter(self.dataset) if self.dataset is not None else None res = filter(lambda o:o is not None, map(self.do_item, samps)) yield from map(self.do_batch, self.chunkify(res)) def new(self, dataset=None, cls=None, **kwargs): if dataset is None: dataset = self.dataset if cls is None: cls = type(self) cur_kwargs = dict(dataset=dataset, num_workers=self.fake_l.num_workers, pin_memory=self.pin_memory, timeout=self.timeout, bs=self.bs, shuffle=self.shuffle, drop_last=self.drop_last, indexed=self.indexed, device=self.device) for n in self._methods: o = getattr(self, n) if not isinstance(o, MethodType): cur_kwargs[n] = o return cls(**merge(cur_kwargs, kwargs)) @property def prebatched(self): return self.bs is None def do_item(self, s): try: return self.after_item(self.create_item(s)) except SkipItemException: return None def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last) def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs)) def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1)) def retain(self, res, b): return retain_types(res, b[0] if is_listy(b) else b) def create_item(self, s): return next(self.it) if s is None else self.dataset[s] def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) def to(self, device): self.device = device def one_batch(self): if self.n is not None and len(self)==0: raise ValueError(f&#39;This DataLoader does not contain any batches&#39;) with self.fake_l.no_multiproc(): res = first(self) if hasattr(self, &#39;it&#39;): delattr(self, &#39;it&#39;) return res . Arguments to DataLoader: . dataset: dataset from which to load the data. Can be either map-style or iterable-style dataset. | bs (int): how many samples per batch to load (if batch_size is provided then batch_size will override bs). If bs=None, then it is assumed that dataset.__getitem__ returns a batch. | num_workers (int): how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. | pin_memory (bool): If True, the data loader will copy Tensors into CUDA pinned memory before returning them. | timeout (float&gt;0): the timeout value in seconds for collecting a batch from workers. | batch_size (int): It is only provided for PyTorch compatibility. Use bs. | shuffle (bool): If True, then data is shuffled every time dataloader is fully read/iterated. | drop_last (bool): If True, then the last incomplete batch is dropped. | indexed (bool): Set to False, if you are using iterable-style dataset. Otherwise it is set to True by default. | n (int): Defaults to len(dataset). If you are using iterable-style dataset, you can specify the size of batch using n. | device (torch.device): Defaults to default_device() which is CUDA by default. You can specify device as `torch.device(&#39;cpu&#39;). | . Override item and use the default infinite sampler to get a stream of unknown length (stop() when you want to stop the stream). . class RandDL(DataLoader): # just think that create item defines how many batches you want to create def create_item(self, s): r = random.random() return r if r&lt;0.95 else stop() L(RandDL()) . (#2) [0.7845764769109268,0.07663069024469027] . L(RandDL(bs=4, drop_last=True)) . (#5) [tensor([0.4496, 0.1020, 0.7749, 0.2346], dtype=torch.float64),tensor([0.3137, 0.0669, 0.2633, 0.6447], dtype=torch.float64),tensor([0.1578, 0.7143, 0.7018, 0.3614], dtype=torch.float64),tensor([0.0818, 0.8804, 0.0260, 0.1141], dtype=torch.float64),tensor([0.8457, 0.4684, 0.6813, 0.5376], dtype=torch.float64)] . aa = L(torch.randn(3,2,2),torch.randn(1,2)) aa # map(len) 得到每一个个体的len信息 aa.map(len) . (#2) [tensor([[[-0.1817, 0.8239], [-1.2745, 0.2690]], [[-2.4169, -0.0737], [-0.5183, -0.2426]], [[-0.5382, -0.8570], [-0.3183, -1.3729]]]),tensor([[ 0.3762, -0.1435]])] . (#2) [3,1] . # generate n samples, and each len of the sample is 4 L(RandDL(bs=4, drop_last=True)).map(len) . (#19) [4,4,4,4,4,4,4,4,4,4...] . dl = RandDL(bs=4, num_workers=4, drop_last=True) dl aa = L(dl) aa aa.map(len) . &lt;__main__.RandDL at 0x7ff490c795e0&gt; . (#6) [tensor([7.9808e-01, 3.3119e-04, 6.3444e-01, 4.4250e-01], dtype=torch.float64),tensor([0.3784, 0.7446, 0.4139, 0.4271], dtype=torch.float64),tensor([0.0310, 0.9253, 0.8902, 0.7117], dtype=torch.float64),tensor([0.6363, 0.0280, 0.4431, 0.4497], dtype=torch.float64),tensor([0.2198, 0.9301, 0.2775, 0.5392], dtype=torch.float64),tensor([0.9400, 0.6906, 0.3483, 0.1497], dtype=torch.float64)] . (#6) [4,4,4,4,4,4] . test_eq(dl.fake_l.num_workers, 4) with dl.fake_l.no_multiproc(): test_eq(dl.fake_l.num_workers, 0) L(dl).map(len) test_eq(dl.fake_l.num_workers, 4) . (#3) [4,4,4] . def _rand_item(s): r = random.random() return r if r&lt;0.95 else stop() L(DataLoader(create_item=_rand_item)) . (#19) [0.6349563676454735,0.7146332101602991,0.8141618453401647,0.4520649933251427,0.9361665561726571,0.6025762046797407,0.8542014056058742,0.1619398819056156,0.3453745719035911,0.21838379481215286...] . If you don&#39;t set bs, then dataset is assumed to provide an iterator or a __getitem__ that returns a batch. . ds1 = DataLoader(letters) test_eq(L(ds1), letters) test_eq(len(ds1), 26) test_shuffled(L(DataLoader(letters, shuffle=True)), letters) ds1 = DataLoader(letters, indexed=False) test_eq(L(ds1), letters) test_eq(len(ds1), 26) t2 = L(tensor([0,1,2]),tensor([3,4,5])) ds2 = DataLoader(t2) test_eq_type(L(ds2), t2) t3 = L(array([0,1,2]),array([3,4,5])) ds3 = DataLoader(t3) test_eq_type(L(ds3), t3.map(tensor)) ds4 = DataLoader(t3, create_batch=noop, after_iter=lambda: setattr(t3, &#39;f&#39;, 1)) test_eq_type(L(ds4), t3) test_eq(t3.f, 1) . If you do set bs, then dataset is assumed to provide an iterator or a __getitem__ that returns a single item of a batch. . def twoepochs(d): return &#39; &#39;.join(&#39;&#39;.join(list(o)) for _ in range(2) for o in d) . ds1 = DataLoader(letters, bs=4, drop_last=True, num_workers=0) test_eq(twoepochs(ds1), &#39;abcd efgh ijkl mnop qrst uvwx abcd efgh ijkl mnop qrst uvwx&#39;) ds1 = DataLoader(letters,4,num_workers=2) test_eq(twoepochs(ds1), &#39;abcd efgh ijkl mnop qrst uvwx yz abcd efgh ijkl mnop qrst uvwx yz&#39;) ds1 = DataLoader(range(12), bs=4, num_workers=3) test_eq_type(L(ds1), L(tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11]))) ds1 = DataLoader([str(i) for i in range(11)], bs=4, after_iter=lambda: setattr(t3, &#39;f&#39;, 2)) test_eq_type(L(ds1), L([&#39;0&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;],[&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;],[&#39;8&#39;,&#39;9&#39;,&#39;10&#39;])) test_eq(t3.f, 2) it = iter(DataLoader(map(noop,range(20)), bs=4, num_workers=1)) test_eq_type([next(it) for _ in range(3)], [tensor([0,1,2,3]),tensor([4,5,6,7]),tensor([8,9,10,11])]) . def addone(s): s+=1 return s . addone(6) . 7 . ds1 = DataLoader(range(12),bs = 4) L(ds1) ds1 = DataLoader(range(12),bs = 4, create_item=addone) L(ds1) ds1 = DataLoader(range(12),bs = 4, after_item=lambda o: o*2) L(ds1) ds1 = DataLoader(range(12),bs = 4, after_item=lambda o: o*2,create_item=addone) L(ds1) ds1 = DataLoader(range(12),bs = 4 ,create_item=addone, after_item=lambda i : i+2,after_batch=lambda o: o*3) L(ds1) # ds1 = DataLoader(range(12),bs = 4, before_batch=lambda o: o-1) # L(ds1) . (#3) [tensor([0, 1, 2, 3]),tensor([4, 5, 6, 7]),tensor([ 8, 9, 10, 11])] . (#3) [tensor([1, 2, 3, 4]),tensor([5, 6, 7, 8]),tensor([ 9, 10, 11, 12])] . (#3) [tensor([0, 2, 4, 6]),tensor([ 8, 10, 12, 14]),tensor([16, 18, 20, 22])] . (#3) [tensor([2, 4, 6, 8]),tensor([10, 12, 14, 16]),tensor([18, 20, 22, 24])] . (#3) [tensor([ 9, 12, 15, 18]),tensor([21, 24, 27, 30]),tensor([33, 36, 39, 42])] . class SleepyDL(list): def __getitem__(self,i): time.sleep(random.random()/50) return super().__getitem__(i) t = SleepyDL(letters) %time test_eq(DataLoader(t, num_workers=0), letters) %time test_eq(DataLoader(t, num_workers=2), letters) %time test_eq(DataLoader(t, num_workers=4), letters) dl = DataLoader(t, shuffle=True, num_workers=1) test_shuffled(L(dl), letters) test_shuffled(L(dl), L(dl)) . CPU times: user 913 µs, sys: 5.14 ms, total: 6.05 ms Wall time: 302 ms CPU times: user 4.15 ms, sys: 30.2 ms, total: 34.4 ms Wall time: 177 ms CPU times: user 8.4 ms, sys: 35.2 ms, total: 43.6 ms Wall time: 142 ms . class SleepyQueue(): &quot;Simulate a queue with varying latency&quot; def __init__(self, q): self.q=q def __iter__(self): while True: time.sleep(random.random()/100) try: yield self.q.get_nowait() except queues.Empty: return q = Queue() for o in range(30): q.put(o) it = SleepyQueue(q) %time test_shuffled(L(DataLoader(it, num_workers=4)), range(30)) . CPU times: user 9.43 ms, sys: 36.3 ms, total: 45.7 ms Wall time: 118 ms . class A(TensorBase): pass for nw in (0,2): t = A(tensor([1,2])) dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw) b = first(dl) len(b) print(b) b[0] test_eq(type(b), A) t = (A(tensor([1,2])),) dl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=nw) b = first(dl) test_eq(type(b[0]), A) . 4 . A([[1, 2], [1, 2], [1, 2], [1, 2]]) . A([1, 2]) . 4 . A([[1, 2], [1, 2], [1, 2], [1, 2]]) . A([1, 2]) . list(DataLoader(list(range(50)),bs=32,shuffle=True,num_workers=3)) . [tensor([30, 18, 29, 38, 43, 25, 23, 1, 0, 22, 13, 9, 27, 47, 16, 3, 15, 7, 19, 32, 45, 42, 48, 41, 10, 11, 6, 14, 20, 31, 39, 26]), tensor([34, 35, 33, 24, 5, 28, 36, 4, 40, 49, 8, 21, 37, 17, 44, 2, 12, 46])] . class A(TensorBase): pass t = A(tensor(1,2)) tdl = DataLoader([t,t,t,t,t,t,t,t], bs=4, num_workers=2, after_batch=to_device) b = first(tdl) test_eq(type(b), A) # Unknown attributes are delegated to `dataset` test_eq(tdl.pop(), tensor(1,2)) . Override get_idxs to return the same index until consumption of the DL. This is intented to test consistent sampling behavior when num_workers&gt;1. . class AdamantDL(DataLoader): def get_idxs(self): r=random.randint(0,self.n-1) return [r] * self.n test_eq(torch.cat(tuple(AdamantDL((list(range(50))),bs=16,num_workers=4))).unique().numel(),1) .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/27/fastai-dataloader.html",
            "relUrl": "/pytorch/fastai/2021/01/27/fastai-dataloader.html",
            "date": " • Jan 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic experiment for deeplearning",
            "content": "from fastcore import * . aa = L(1,2,3) aa[2] . 3 .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai2/2021/01/17/pytorch-basics.html",
            "relUrl": "/pytorch/fastai2/2021/01/17/pytorch-basics.html",
            "date": " • Jan 17, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Transformer Blog",
            "content": "reference links which helps a lot: . [1] http://jalammar.github.io/illustrated-transformer/ | [2] http://peterbloem.nl/blog/transformers | [3] https://nlp.seas.harvard.edu/2018/04/03/attention.html | . import numpy as np import matplotlib.pyplot as plt import torch from torch import nn import torch.nn.functional as F . self-attention method . . . x = torch.randn(2,3,4) raw_weights = torch.bmm(x,x.transpose(1,2)) weights = F.softmax(raw_weights,dim=2) weights . tensor([[[3.7608e-01, 1.4019e-01, 4.8374e-01], [1.3241e-02, 9.8102e-01, 5.7356e-03], [6.7872e-02, 8.5199e-03, 9.2361e-01]], [[8.6212e-01, 1.9809e-02, 1.1807e-01], [6.0491e-04, 9.8575e-01, 1.3647e-02], [1.1610e-01, 4.3942e-01, 4.4448e-01]]]) . y = torch.bmm(weights,x) y . tensor([[[ 0.7534, 0.1771, -0.0526, -0.4511], [ 0.0585, -1.1643, 1.3854, 0.4464], [ 1.2452, 0.3228, -0.4401, -0.9620]], [[-0.8668, 0.0406, 0.0529, -0.2900], [ 1.9601, -0.1198, -0.9540, 0.3403], [ 0.9987, -0.2199, -0.2520, -0.0090]]]) . torch.random.seed() temp = torch.randint(0,2,(2,2)) temp yy = F.softmax(temp.float(),dim=0) yy . 4912394464713312614 . tensor([[1, 1], [0, 1]]) . tensor([[0.7311, 0.5000], [0.2689, 0.5000]]) . . aa = np.random.randint(1,10,(3,4)) aa . array([[2, 9, 7, 7], [2, 3, 3, 4], [2, 2, 8, 9]]) . aa[:,0::2] . array([[2, 7], [2, 3], [2, 8]]) . multi-head attention . class SelfAttentionWide(nn.Module): def __init__(self, emb, heads=8, mask=False): &quot;&quot;&quot; :param emb: :param heads: :param mask: &quot;&quot;&quot; super().__init__() self.emb = emb self.heads = heads self.mask = mask self.tokeys = nn.Linear(emb, emb * heads, bias=False) self.toqueries = nn.Linear(emb, emb * heads, bias=False) self.tovalues = nn.Linear(emb, emb * heads, bias=False) self.unifyheads = nn.Linear(heads * emb, emb) def forward(self, x): b, t, e = x.size() h = self.heads assert e == self.emb, f&#39;Input embedding dim ({e}) should match layer embedding dim ({self.emb})&#39; keys = self.tokeys(x) .view(b, t, h, e) queries = self.toqueries(x).view(b, t, h, e) values = self.tovalues(x) .view(b, t, h, e) # compute scaled dot-product self-attention # - fold heads into the batch dimension keys = keys.transpose(1, 2).contiguous().view(b * h, t, e) queries = queries.transpose(1, 2).contiguous().view(b * h, t, e) values = values.transpose(1, 2).contiguous().view(b * h, t, e) queries = queries / (e ** (1/4)) keys = keys / (e ** (1/4)) # - Instead of dividing the dot products by sqrt(e), we scale the keys and values. # This should be more memory efficient # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) assert dot.size() == (b*h, t, t) if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal mask_(dot, maskval=float(&#39;-inf&#39;), mask_diagonal=False) dot = F.softmax(dot, dim=2) # - dot now has row-wise self-attention probabilities # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, e) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, h * e) return self.unifyheads(out) class SelfAttentionNarrow(nn.Module): def __init__(self, emb, heads=8, mask=False): &quot;&quot;&quot; :param emb: :param heads: :param mask: &quot;&quot;&quot; super().__init__() assert emb % heads == 0, f&#39;Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})&#39; self.emb = emb self.heads = heads self.mask = mask s = emb // heads # - We will break the embedding into `heads` chunks and feed each to a different attention head self.tokeys = nn.Linear(s, s, bias=False) self.toqueries = nn.Linear(s, s, bias=False) self.tovalues = nn.Linear(s, s, bias=False) self.unifyheads = nn.Linear(heads * s, emb) def forward(self, x): b, t, e = x.size() h = self.heads assert e == self.emb, f&#39;Input embedding dim ({e}) should match layer embedding dim ({self.emb})&#39; s = e // h x = x.view(b, t, h, s) keys = self.tokeys(x) queries = self.toqueries(x) values = self.tovalues(x) assert keys.size() == (b, t, h, s) assert queries.size() == (b, t, h, s) assert values.size() == (b, t, h, s) # Compute scaled dot-product self-attention # - fold heads into the batch dimension keys = keys.transpose(1, 2).contiguous().view(b * h, t, s) queries = queries.transpose(1, 2).contiguous().view(b * h, t, s) values = values.transpose(1, 2).contiguous().view(b * h, t, s) queries = queries / (e ** (1/4)) keys = keys / (e ** (1/4)) # - Instead of dividing the dot products by sqrt(e), we scale the keys and values. # This should be more memory efficient # - get dot product of queries and keys, and scale dot = torch.bmm(queries, keys.transpose(1, 2)) assert dot.size() == (b*h, t, t) if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal mask_(dot, maskval=float(&#39;-inf&#39;), mask_diagonal=False) dot = F.softmax(dot, dim=2) # - dot now has row-wise self-attention probabilities # apply the self attention to the values out = torch.bmm(dot, values).view(b, h, t, s) # swap h, t back, unify heads out = out.transpose(1, 2).contiguous().view(b, t, s * h) return self.unifyheads(out) . we can also change the code using einsum which help to short the code part and have a nice execution time https://rockt.github.io/2018/04/30/einsum . class SelfAttentionWideEinsum(nn.Module): def __init__(self, emb, heads=8, mask=False): &quot;&quot;&quot; :param emb: :param heads: :param mask: &quot;&quot;&quot; super().__init__() self.emb = emb self.heads = heads self.mask = mask self.tokeys = nn.Linear(emb, emb * heads, bias=False) self.toqueries = nn.Linear(emb, emb * heads, bias=False) self.tovalues = nn.Linear(emb, emb * heads, bias=False) self.unifyheads = nn.Linear(heads * emb, emb) def forward_einsum(self, x): b, t, e = x.size() h = self.heads keys = self.tokeys(x).view(b, t, h, e) queries = self.toqueries(x).view(b, t, h, e) values = self.tovalues(x).view(b, t, h, e) dot = torch.einsum(&#39;bthe,bihe-&gt;bhti&#39;, queries, keys) / math.sqrt(e) dot = F.softmax(dot, dim=-1) out = torch.einsum(&#39;bhtd,bdhe-&gt;bthe&#39;, dot, values) # we can move reshape of weights to init; I left it here just to compare with the original implementation out = torch.einsum(&#39;bthe,khe-&gt;btk&#39;, out, self.unifyheads.weight.view(e,h,e)) return out + self.unifyheads.bias . transformer structure . . class TransformerBlock(nn.Module): def __init__(self, emb, heads, mask, seq_length, ff_hidden_mult=4, dropout=0.0, wide=True): super().__init__() self.attention = SelfAttentionWide(emb, heads=heads, mask=mask) if wide else SelfAttentionNarrow(emb, heads=heads, mask=mask) self.mask = mask self.norm1 = nn.LayerNorm(emb) self.norm2 = nn.LayerNorm(emb) &#39;&#39;&#39; We’ve made the relatively arbitrary choice of making the hidden layer of the feedforward 4 times as big as the input and output. Smaller values may work as well, and save memory, but it should be bigger than the input/output layers. &#39;&#39;&#39; self.ff = nn.Sequential( nn.Linear(emb, ff_hidden_mult * emb), nn.ReLU(), nn.Linear(ff_hidden_mult * emb, emb) ) self.do = nn.Dropout(dropout) def forward(self, x): attended = self.attention(x) x = self.norm1(attended + x) x = self.do(x) fedforward = self.ff(x) x = self.norm2(fedforward + x) x = self.do(x) return x . position embedding . def d(tensor=None): &quot;&quot;&quot; Returns a device string either for the best available device, or for the device corresponding to the argument :param tensor: :return: &quot;&quot;&quot; if tensor is None: return &#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39; return &#39;cuda&#39; if tensor.is_cuda else &#39;cpu&#39; . temp = torch.randn(2,3,4) temp . tensor([[[-0.9651, 1.4951, -0.0168, 0.7085], [-1.9934, -0.5921, -0.3682, -0.8308], [ 1.0251, -0.0033, -1.4288, 0.4307]], [[-0.1145, 0.4717, -0.5771, 0.8367], [ 0.8415, -0.2907, 2.7137, -0.3131], [ 1.2084, 0.0839, -0.4571, -0.1604]]]) . &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 6 &gt;&gt;&gt; embedding = nn.Embedding(10, 6) &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.LongTensor([[1,2,3]]) &gt;&gt;&gt; embedding(input) . tensor([[[-1.0416, -1.2013, -1.1024, -0.2295, 0.7987, 0.5698], [-0.9966, 0.5302, -0.6908, -2.4040, -0.1549, -0.0050], [ 0.1405, -0.4664, -0.2933, -0.0160, 0.0548, -0.3741]]], grad_fn=&lt;EmbeddingBackward&gt;) . temp[None,:,:].shape . torch.Size([1, 2, 3, 4]) . b,t,e = 3,5,6 position = nn.Embedding(10,6)(torch.arange(t))[None, :, :].expand(b, t, e) position . tensor([[[-1.6559, -0.3209, 2.2730, 0.3641, -1.5789, -1.0718], [-0.3217, 0.2345, 1.8767, -0.8459, -1.0136, 0.1944], [ 0.2643, -1.5120, -0.1799, 1.8587, 0.7489, 0.0663], [-0.2499, 0.6199, 0.6119, -0.1948, -1.2249, -0.9786], [-0.0888, 1.4573, -0.0139, -1.5792, 1.0114, -0.6898]], [[-1.6559, -0.3209, 2.2730, 0.3641, -1.5789, -1.0718], [-0.3217, 0.2345, 1.8767, -0.8459, -1.0136, 0.1944], [ 0.2643, -1.5120, -0.1799, 1.8587, 0.7489, 0.0663], [-0.2499, 0.6199, 0.6119, -0.1948, -1.2249, -0.9786], [-0.0888, 1.4573, -0.0139, -1.5792, 1.0114, -0.6898]], [[-1.6559, -0.3209, 2.2730, 0.3641, -1.5789, -1.0718], [-0.3217, 0.2345, 1.8767, -0.8459, -1.0136, 0.1944], [ 0.2643, -1.5120, -0.1799, 1.8587, 0.7489, 0.0663], [-0.2499, 0.6199, 0.6119, -0.1948, -1.2249, -0.9786], [-0.0888, 1.4573, -0.0139, -1.5792, 1.0114, -0.6898]]], grad_fn=&lt;ExpandBackward&gt;) . position encoding . # Code from https://www.tensorflow.org/tutorials/text/transformer def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) return pos * angle_rates def positional_encoding(position, d_model): angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model) # apply sin to even indices in the array; 2i angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # apply cos to odd indices in the array; 2i+1 angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) pos_encoding = angle_rads[np.newaxis, ...] return pos_encoding . tokens = 10 dimensions = 64 pos_encoding = positional_encoding(tokens, dimensions) print (pos_encoding.shape) plt.figure(figsize=(12,8)) plt.pcolormesh(pos_encoding[0], cmap=&#39;viridis&#39;) plt.xlabel(&#39;Embedding Dimensions&#39;) plt.xlim((0, dimensions)) plt.ylim((tokens,0)) plt.ylabel(&#39;Token Position&#39;) plt.colorbar() plt.show() . (1, 10, 64) . &lt;Figure size 864x576 with 0 Axes&gt; . &lt;matplotlib.collections.QuadMesh at 0x7f943d17ac50&gt; . Text(0.5, 0, &#39;Embedding Dimensions&#39;) . (0.0, 64.0) . (10.0, 0.0) . Text(0, 0.5, &#39;Token Position&#39;) . &lt;matplotlib.colorbar.Colorbar at 0x7f943d12c5c0&gt; . Transformer structure . class Transformer(nn.Module): def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes): super().__init__() self.num_tokens = num_tokens self.token_emb = nn.Embedding(num_tokens, k) self.pos_emb = nn.Embedding(seq_length, k) # The sequence of transformer blocks that does all the # heavy lifting tblocks = [] for i in range(depth): tblocks.append(TransformerBlock(k=k, heads=heads)) self.tblocks = nn.Sequential(*tblocks) # Maps the final output sequence to class logits self.toprobs = nn.Linear(k, num_classes) def forward(self, x): &quot;&quot;&quot; :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary). :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes). &quot;&quot;&quot; # generate token embeddings tokens = self.token_emb(x) b, t, k = tokens.size() # generate position embeddings positions = torch.arange(t) positions = self.pos_emb(positions)[None, :, :].expand(b, t, k) x = tokens + positions x = self.tblocks(x) # Average-pool over the t dimension and project to class # probabilities x = self.toprobs(x.mean(dim=1)) return F.log_softmax(x, dim=1) . mask . . https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices | . iu1 = np.triu_indices(4) iu2 = np.triu_indices(4,2) iu1 # iu2 . (array([0, 0, 0, 0, 1, 1, 1, 2, 2, 3]), array([0, 1, 2, 3, 1, 2, 3, 2, 3, 3])) . a = np.arange(16).reshape(4,4) a[iu1] . array([ 0, 1, 2, 3, 5, 6, 7, 10, 11, 15]) . a . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) . a[iu2] . array([2, 3, 7]) . iu1 = torch.triu_indices(4,4) a = torch.arange(16).view(4,4) a iu1 . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]]) . tensor([[0, 0, 0, 0, 1, 1, 1, 2, 2, 3], [0, 1, 2, 3, 1, 2, 3, 2, 3, 3]]) . a[iu1[0],iu1[1]] . tensor([ 0, 1, 2, 3, 5, 6, 7, 10, 11, 15]) . # mask function def mask_(matrices, maskval=0.0, mask_diagonal=True): &quot;&quot;&quot; Masks out all values in the given batch of matrices where i &lt;= j holds, i &lt; j if mask_diagonal is false In place operation :param tns: :return: &quot;&quot;&quot; b, h, w = matrices.size() indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1) matrices[:, indices[0], indices[1]] = maskval . queries = torch.randn(1,3,3) keys = torch.randn(1,3,3) t = 3 dot = torch.bmm(queries, keys.transpose(1, 2)) indices = torch.triu_indices(t, t, offset=1) dot[:, indices[0], indices[1]] = float(&#39;-inf&#39;) dot = F.softmax(dot, dim=2) indices dot dot[:, indices[0], indices[1]] . tensor([[0, 0, 1], [1, 2, 2]]) . tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00], [9.0848e-01, 9.1524e-02, 0.0000e+00], [2.5938e-04, 9.9937e-01, 3.6845e-04]]]) . tensor([[0., 0., 0.]]) .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html",
            "relUrl": "/pytorch/fastai/2021/01/10/transformer-blog.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Understranding HRNET with code",
            "content": ". . Hrnet + ocr module is as follows, all the codes borrow from : . https://github.com/HRNet/HRNet-Semantic-Segmentation/blob/HRNet-OCR/lib/models/seg_hrnet_ocr.py | https://github.com/openseg-group/openseg.pytorch | base part . from __future__ import absolute_import from __future__ import division from __future__ import print_function import os import logging import functools import numpy as np import torch import torch.nn as nn import torch._utils import torch.nn.functional as F import sys sys.path.append( &#39;/home/ubuntu/ds/segmentation/HRNet-Semantic-Segmentation/lib/models/&#39;) # from .bn_helper import BatchNorm2d, BatchNorm2d_class, relu_inplace from bn_helper import BatchNorm2d, BatchNorm2d_class, relu_inplace ALIGN_CORNERS = True BN_MOMENTUM = 0.1 logger = logging.getLogger(__name__) sys.path.append(&#39;/home/ubuntu/ds/segmentation/HRNet-Semantic-Segmentation/lib/&#39;) from config import config from config import update_config from config.default import _C as cfg # config file path = &#39;/home/ubuntu/ds/segmentation/HRNet-Semantic-Segmentation/experiments/cityscapes/seg_hrnet_ocr_w48_trainval_512x1024_sgd_lr1e-2_wd5e-4_bs_12_epoch484.yaml&#39; config.merge_from_file(path) class ModuleHelper: @staticmethod def BNReLU(num_features, bn_type=None, **kwargs): return nn.Sequential( BatchNorm2d(num_features, **kwargs), nn.ReLU() ) @staticmethod def BatchNorm2d(*args, **kwargs): return BatchNorm2d def conv3x3(in_planes, out_planes, stride=1): &quot;&quot;&quot;3x3 convolution with padding&quot;&quot;&quot; return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False) . config . CfgNode({&#39;OUTPUT_DIR&#39;: &#39;output&#39;, &#39;LOG_DIR&#39;: &#39;log&#39;, &#39;GPUS&#39;: (0,), &#39;WORKERS&#39;: 1, &#39;PRINT_FREQ&#39;: 10, &#39;AUTO_RESUME&#39;: False, &#39;PIN_MEMORY&#39;: True, &#39;RANK&#39;: 0, &#39;CUDNN&#39;: CfgNode({&#39;BENCHMARK&#39;: True, &#39;DETERMINISTIC&#39;: False, &#39;ENABLED&#39;: True}), &#39;MODEL&#39;: CfgNode({&#39;NAME&#39;: &#39;seg_hrnet_ocr&#39;, &#39;PRETRAINED&#39;: &#39;pretrained_models/hrnetv2_w48_imagenet_pretrained.pth&#39;, &#39;ALIGN_CORNERS&#39;: True, &#39;NUM_OUTPUTS&#39;: 2, &#39;EXTRA&#39;: CfgNode({&#39;FINAL_CONV_KERNEL&#39;: 1, &#39;STAGE1&#39;: CfgNode({&#39;NUM_MODULES&#39;: 1, &#39;NUM_RANCHES&#39;: 1, &#39;BLOCK&#39;: &#39;BOTTLENECK&#39;, &#39;NUM_BLOCKS&#39;: [4], &#39;NUM_CHANNELS&#39;: [64], &#39;FUSE_METHOD&#39;: &#39;SUM&#39;}), &#39;STAGE2&#39;: CfgNode({&#39;NUM_MODULES&#39;: 1, &#39;NUM_BRANCHES&#39;: 2, &#39;BLOCK&#39;: &#39;BASIC&#39;, &#39;NUM_BLOCKS&#39;: [4, 4], &#39;NUM_CHANNELS&#39;: [48, 96], &#39;FUSE_METHOD&#39;: &#39;SUM&#39;}), &#39;STAGE3&#39;: CfgNode({&#39;NUM_MODULES&#39;: 4, &#39;NUM_BRANCHES&#39;: 3, &#39;BLOCK&#39;: &#39;BASIC&#39;, &#39;NUM_BLOCKS&#39;: [4, 4, 4], &#39;NUM_CHANNELS&#39;: [48, 96, 192], &#39;FUSE_METHOD&#39;: &#39;SUM&#39;}), &#39;STAGE4&#39;: CfgNode({&#39;NUM_MODULES&#39;: 3, &#39;NUM_BRANCHES&#39;: 4, &#39;BLOCK&#39;: &#39;BASIC&#39;, &#39;NUM_BLOCKS&#39;: [4, 4, 4, 4], &#39;NUM_CHANNELS&#39;: [48, 96, 192, 384], &#39;FUSE_METHOD&#39;: &#39;SUM&#39;})}), &#39;OCR&#39;: CfgNode({&#39;MID_CHANNELS&#39;: 512, &#39;KEY_CHANNELS&#39;: 256, &#39;DROPOUT&#39;: 0.05, &#39;SCALE&#39;: 1})}), &#39;LOSS&#39;: CfgNode({&#39;USE_OHEM&#39;: False, &#39;OHEMTHRES&#39;: 0.9, &#39;OHEMKEEP&#39;: 131072, &#39;CLASS_BALANCE&#39;: False, &#39;BALANCE_WEIGHTS&#39;: [0.4, 1]}), &#39;DATASET&#39;: CfgNode({&#39;ROOT&#39;: &#39;&#39;, &#39;DATASET&#39;: &#39;cityscapesEXTRA_TRAIN_SET&#39;, &#39;NUM_CLASSES&#39;: 19, &#39;TRAIN_SET&#39;: &#39;list/cityscapes/trainval.lst&#39;, &#39;EXTRA_TRAIN_SET&#39;: &#39;&#39;, &#39;TEST_SET&#39;: &#39;list/cityscapes/val.lst&#39;}), &#39;TRAIN&#39;: CfgNode({&#39;FREEZE_LAYERS&#39;: &#39;&#39;, &#39;FREEZE_EPOCHS&#39;: -1, &#39;NONBACKBONE_KEYWORDS&#39;: [], &#39;NONBACKBONE_MULT&#39;: 10, &#39;IMAGE_SIZE&#39;: [1024, 512], &#39;BASE_SIZE&#39;: 2048, &#39;DOWNSAMPLERATE&#39;: 1, &#39;FLIP&#39;: True, &#39;MULTI_SCALE&#39;: True, &#39;SCALE_FACTOR&#39;: 16, &#39;RANDOM_BRIGHTNESS&#39;: False, &#39;RANDOM_BRIGHTNESS_SHIFT_VALUE&#39;: 10, &#39;LR_FACTOR&#39;: 0.1, &#39;LR_STEP&#39;: [90, 110], &#39;LR&#39;: 0.01, &#39;EXTRA_LR&#39;: 0.001, &#39;OPTIMIZER&#39;: &#39;sgd&#39;, &#39;MOMENTUM&#39;: 0.9, &#39;WD&#39;: 0.0005, &#39;NESTEROV&#39;: False, &#39;IGNORE_LABEL&#39;: 255, &#39;BEGIN_EPOCH&#39;: 0, &#39;END_EPOCH&#39;: 484, &#39;EXTRA_EPOCH&#39;: 0, &#39;RESUME&#39;: True, &#39;BATCH_SIZE_PER_GPU&#39;: 3, &#39;SHUFFLE&#39;: True, &#39;NUM_SAMPLES&#39;: 0}), &#39;TEST&#39;: CfgNode({&#39;IMAGE_SIZE&#39;: [2048, 1024], &#39;BASE_SIZE&#39;: 2048, &#39;BATCH_SIZE_PER_GPU&#39;: 4, &#39;NUM_SAMPLES&#39;: 0, &#39;MODEL_FILE&#39;: &#39;&#39;, &#39;FLIP_TEST&#39;: False, &#39;MULTI_SCALE&#39;: False, &#39;SCALE_LIST&#39;: [1], &#39;OUTPUT_INDEX&#39;: -1}), &#39;DEBUG&#39;: CfgNode({&#39;DEBUG&#39;: False, &#39;SAVE_BATCH_IMAGES_GT&#39;: False, &#39;SAVE_BATCH_IMAGES_PRED&#39;: False, &#39;SAVE_HEATMAPS_GT&#39;: False, &#39;SAVE_HEATMAPS_PRED&#39;: False})}) . for i in range(1,4): print(i) . 1 2 3 . OCR module . class SpatialGather_Module(nn.Module): &quot;&quot;&quot; Aggregate the context features according to the initial predicted probability distribution. Employ the soft-weighted method to aggregate the context. &quot;&quot;&quot; def __init__(self, cls_num=0, scale=1): super(SpatialGather_Module, self).__init__() self.cls_num = cls_num self.scale = scale def forward(self, feats, probs): batch_size, c, h, w = probs.size(0), probs.size( 1), probs.size(2), probs.size(3) probs = probs.view(batch_size, c, -1) feats = feats.view(batch_size, feats.size(1), -1) feats = feats.permute(0, 2, 1) # batch x hw x c probs = F.softmax(self.scale * probs, dim=2) # batch x k x hw ocr_context = torch.matmul(probs, feats) .permute(0, 2, 1).unsqueeze(3) # batch x k x c return ocr_context class _ObjectAttentionBlock(nn.Module): &#39;&#39;&#39; The basic implementation for object context block Input: N X C X H X W Parameters: in_channels : the dimension of the input feature map key_channels : the dimension after the key/query transform scale : choose the scale to downsample the input feature maps (save memory cost) bn_type : specify the bn type Return: N X C X H X W &#39;&#39;&#39; def __init__(self, in_channels, key_channels, scale=1, bn_type=None): super(_ObjectAttentionBlock, self).__init__() self.scale = scale self.in_channels = in_channels self.key_channels = key_channels self.pool = nn.MaxPool2d(kernel_size=(scale, scale)) self.f_pixel = nn.Sequential( nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type), nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type), ) self.f_object = nn.Sequential( nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type), nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type), ) self.f_down = nn.Sequential( nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type), ) self.f_up = nn.Sequential( nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels, kernel_size=1, stride=1, padding=0, bias=False), ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type), ) def forward(self, x, proxy): batch_size, h, w = x.size(0), x.size(2), x.size(3) if self.scale &gt; 1: x = self.pool(x) query = self.f_pixel(x).view(batch_size, self.key_channels, -1) query = query.permute(0, 2, 1) key = self.f_object(proxy).view(batch_size, self.key_channels, -1) value = self.f_down(proxy).view(batch_size, self.key_channels, -1) value = value.permute(0, 2, 1) sim_map = torch.matmul(query, key) sim_map = (self.key_channels**-.5) * sim_map sim_map = F.softmax(sim_map, dim=-1) # add bg context ... context = torch.matmul(sim_map, value) context = context.permute(0, 2, 1).contiguous() context = context.view(batch_size, self.key_channels, *x.size()[2:]) context = self.f_up(context) if self.scale &gt; 1: context = F.interpolate(input=context, size=( h, w), mode=&#39;bilinear&#39;, align_corners=ALIGN_CORNERS) return context class ObjectAttentionBlock2D(_ObjectAttentionBlock): def __init__(self, in_channels, key_channels, scale=1, bn_type=None): super(ObjectAttentionBlock2D, self).__init__(in_channels, key_channels, scale, bn_type=bn_type) class SpatialOCR_Module(nn.Module): &quot;&quot;&quot; Implementation of the OCR module: We aggregate the global object representation to update the representation for each pixel. &quot;&quot;&quot; def __init__(self, in_channels, key_channels, out_channels, scale=1, dropout=0.1, bn_type=None): super(SpatialOCR_Module, self).__init__() self.object_context_block = ObjectAttentionBlock2D(in_channels, key_channels, scale, bn_type) _in_channels = 2 * in_channels self.conv_bn_dropout = nn.Sequential( nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False), ModuleHelper.BNReLU(out_channels, bn_type=bn_type), nn.Dropout2d(dropout) ) def forward(self, feats, proxy_feats): context = self.object_context_block(feats, proxy_feats) output = self.conv_bn_dropout(torch.cat([context, feats], 1)) return output . Basic and Bottleneck Module . class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.conv2 = conv3x3(planes, planes) self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) if self.downsample is not None: residual = self.downsample(x) out = out + residual out = self.relu(out) return out class Bottleneck(nn.Module): expansion = 4 def __init__(self, inplanes, planes, stride=1, downsample=None): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False) self.bn1 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = BatchNorm2d(planes, momentum=BN_MOMENTUM) self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False) self.bn3 = BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.relu(out) out = self.conv3(out) out = self.bn3(out) if self.downsample is not None: residual = self.downsample(x) out = out + residual out = self.relu(out) return out . HighResolution Module . class HighResolutionModule(nn.Module): def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, fuse_method, multi_scale_output=True): super(HighResolutionModule, self).__init__() self._check_branches( num_branches, blocks, num_blocks, num_inchannels, num_channels) self.num_inchannels = num_inchannels self.fuse_method = fuse_method self.num_branches = num_branches self.multi_scale_output = multi_scale_output self.branches = self._make_branches( num_branches, blocks, num_blocks, num_channels) self.fuse_layers = self._make_fuse_layers() self.relu = nn.ReLU(inplace=relu_inplace) def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels): if num_branches != len(num_blocks): error_msg = &#39;NUM_BRANCHES({}) &lt;&gt; NUM_BLOCKS({})&#39;.format( num_branches, len(num_blocks)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_channels): error_msg = &#39;NUM_BRANCHES({}) &lt;&gt; NUM_CHANNELS({})&#39;.format( num_branches, len(num_channels)) logger.error(error_msg) raise ValueError(error_msg) if num_branches != len(num_inchannels): error_msg = &#39;NUM_BRANCHES({}) &lt;&gt; NUM_INCHANNELS({})&#39;.format( num_branches, len(num_inchannels)) logger.error(error_msg) raise ValueError(error_msg) def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1): downsample = None if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion: downsample = nn.Sequential( nn.Conv2d(self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion, kernel_size=1, stride=stride, bias=False), BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM), ) layers = [] layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample)) self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion for i in range(1, num_blocks[branch_index]): layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index])) return nn.Sequential(*layers) def _make_branches(self, num_branches, block, num_blocks, num_channels): branches = [] for i in range(num_branches): branches.append( self._make_one_branch(i, block, num_blocks, num_channels)) return nn.ModuleList(branches) def _make_fuse_layers(self): if self.num_branches == 1: return None num_branches = self.num_branches num_inchannels = self.num_inchannels fuse_layers = [] for i in range(num_branches if self.multi_scale_output else 1): fuse_layer = [] for j in range(num_branches): if j &gt; i: fuse_layer.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False), BatchNorm2d(num_inchannels[i], momentum=BN_MOMENTUM))) elif j == i: fuse_layer.append(None) else: conv3x3s = [] for k in range(i-j): if k == i - j - 1: num_outchannels_conv3x3 = num_inchannels[i] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM))) else: num_outchannels_conv3x3 = num_inchannels[j] conv3x3s.append(nn.Sequential( nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False), BatchNorm2d(num_outchannels_conv3x3, momentum=BN_MOMENTUM), nn.ReLU(inplace=relu_inplace))) fuse_layer.append(nn.Sequential(*conv3x3s)) fuse_layers.append(nn.ModuleList(fuse_layer)) return nn.ModuleList(fuse_layers) def get_num_inchannels(self): return self.num_inchannels def forward(self, x): if self.num_branches == 1: return [self.branches[0](x[0])] for i in range(self.num_branches): x[i] = self.branches[i](x[i]) x_fuse = [] for i in range(len(self.fuse_layers)): y = x[0] if i == 0 else self.fuse_layers[i][0](x[0]) for j in range(1, self.num_branches): if i == j: y = y + x[j] elif j &gt; i: width_output = x[i].shape[-1] height_output = x[i].shape[-2] y = y + F.interpolate( self.fuse_layers[i][j](x[j]), size=[height_output, width_output], mode=&#39;bilinear&#39;, align_corners=ALIGN_CORNERS) else: y = y + self.fuse_layers[i][j](x[j]) x_fuse.append(self.relu(y)) return x_fuse blocks_dict = { &#39;BASIC&#39;: BasicBlock, &#39;BOTTLENECK&#39;: Bottleneck } . class HighResolutionNet(nn.Module): def __init__(self, config, **kwargs): global ALIGN_CORNERS extra = config.MODEL.EXTRA super(HighResolutionNet, self).__init__() ALIGN_CORNERS = config.MODEL.ALIGN_CORNERS # stem net self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False) self.bn1 = BatchNorm2d(64, momentum=BN_MOMENTUM) self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False) self.bn2 = BatchNorm2d(64, momentum=BN_MOMENTUM) self.relu = nn.ReLU(inplace=relu_inplace) self.stage1_cfg = extra[&#39;STAGE1&#39;] num_channels = self.stage1_cfg[&#39;NUM_CHANNELS&#39;][0] block = blocks_dict[self.stage1_cfg[&#39;BLOCK&#39;]] num_blocks = self.stage1_cfg[&#39;NUM_BLOCKS&#39;][0] self.layer1 = self._make_layer(block, 64, num_channels, num_blocks) stage1_out_channel = block.expansion*num_channels self.stage2_cfg = extra[&#39;STAGE2&#39;] num_channels = self.stage2_cfg[&#39;NUM_CHANNELS&#39;] block = blocks_dict[self.stage2_cfg[&#39;BLOCK&#39;]] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition1 = self._make_transition_layer( [stage1_out_channel], num_channels) self.stage2, pre_stage_channels = self._make_stage( self.stage2_cfg, num_channels) self.stage3_cfg = extra[&#39;STAGE3&#39;] num_channels = self.stage3_cfg[&#39;NUM_CHANNELS&#39;] block = blocks_dict[self.stage3_cfg[&#39;BLOCK&#39;]] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition2 = self._make_transition_layer( pre_stage_channels, num_channels) self.stage3, pre_stage_channels = self._make_stage( self.stage3_cfg, num_channels) self.stage4_cfg = extra[&#39;STAGE4&#39;] num_channels = self.stage4_cfg[&#39;NUM_CHANNELS&#39;] block = blocks_dict[self.stage4_cfg[&#39;BLOCK&#39;]] num_channels = [ num_channels[i] * block.expansion for i in range(len(num_channels))] self.transition3 = self._make_transition_layer( pre_stage_channels, num_channels) self.stage4, pre_stage_channels = self._make_stage( self.stage4_cfg, num_channels, multi_scale_output=True) last_inp_channels = np.int(np.sum(pre_stage_channels)) ocr_mid_channels = config.MODEL.OCR.MID_CHANNELS ocr_key_channels = config.MODEL.OCR.KEY_CHANNELS self.conv3x3_ocr = nn.Sequential( nn.Conv2d(last_inp_channels, ocr_mid_channels, kernel_size=3, stride=1, padding=1), BatchNorm2d(ocr_mid_channels), nn.ReLU(inplace=relu_inplace), ) self.ocr_gather_head = SpatialGather_Module(config.DATASET.NUM_CLASSES) self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels, key_channels=ocr_key_channels, out_channels=ocr_mid_channels, scale=1, dropout=0.05, ) self.cls_head = nn.Conv2d( ocr_mid_channels, config.DATASET.NUM_CLASSES, kernel_size=1, stride=1, padding=0, bias=True) self.aux_head = nn.Sequential( nn.Conv2d(last_inp_channels, last_inp_channels, kernel_size=1, stride=1, padding=0), BatchNorm2d(last_inp_channels), nn.ReLU(inplace=relu_inplace), nn.Conv2d(last_inp_channels, config.DATASET.NUM_CLASSES, kernel_size=1, stride=1, padding=0, bias=True) ) def _make_transition_layer( self, num_channels_pre_layer, num_channels_cur_layer): num_branches_cur = len(num_channels_cur_layer) num_branches_pre = len(num_channels_pre_layer) transition_layers = [] for i in range(num_branches_cur): if i &lt; num_branches_pre: if num_channels_cur_layer[i] != num_channels_pre_layer[i]: transition_layers.append(nn.Sequential( nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False), BatchNorm2d( num_channels_cur_layer[i], momentum=BN_MOMENTUM), nn.ReLU(inplace=relu_inplace))) else: transition_layers.append(None) else: conv3x3s = [] for j in range(i+1-num_branches_pre): inchannels = num_channels_pre_layer[-1] outchannels = num_channels_cur_layer[i] if j == i-num_branches_pre else inchannels conv3x3s.append(nn.Sequential( nn.Conv2d( inchannels, outchannels, 3, 2, 1, bias=False), BatchNorm2d(outchannels, momentum=BN_MOMENTUM), nn.ReLU(inplace=relu_inplace))) transition_layers.append(nn.Sequential(*conv3x3s)) return nn.ModuleList(transition_layers) def _make_layer(self, block, inplanes, planes, blocks, stride=1): downsample = None if stride != 1 or inplanes != planes * block.expansion: downsample = nn.Sequential( nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False), BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM), ) layers = [] layers.append(block(inplanes, planes, stride, downsample)) inplanes = planes * block.expansion for i in range(1, blocks): layers.append(block(inplanes, planes)) return nn.Sequential(*layers) def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True): num_modules = layer_config[&#39;NUM_MODULES&#39;] num_branches = layer_config[&#39;NUM_BRANCHES&#39;] num_blocks = layer_config[&#39;NUM_BLOCKS&#39;] num_channels = layer_config[&#39;NUM_CHANNELS&#39;] block = blocks_dict[layer_config[&#39;BLOCK&#39;]] fuse_method = layer_config[&#39;FUSE_METHOD&#39;] modules = [] for i in range(num_modules): # multi_scale_output is only used last module if not multi_scale_output and i == num_modules - 1: reset_multi_scale_output = False else: reset_multi_scale_output = True modules.append( HighResolutionModule(num_branches, block, num_blocks, num_inchannels, num_channels, fuse_method, reset_multi_scale_output) ) num_inchannels = modules[-1].get_num_inchannels() return nn.Sequential(*modules), num_inchannels def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.conv2(x) x = self.bn2(x) x = self.relu(x) x = self.layer1(x) x_list = [] for i in range(self.stage2_cfg[&#39;NUM_BRANCHES&#39;]): if self.transition1[i] is not None: x_list.append(self.transition1[i](x)) else: x_list.append(x) y_list = self.stage2(x_list) x_list = [] for i in range(self.stage3_cfg[&#39;NUM_BRANCHES&#39;]): if self.transition2[i] is not None: if i &lt; self.stage2_cfg[&#39;NUM_BRANCHES&#39;]: x_list.append(self.transition2[i](y_list[i])) else: x_list.append(self.transition2[i](y_list[-1])) else: x_list.append(y_list[i]) y_list = self.stage3(x_list) x_list = [] for i in range(self.stage4_cfg[&#39;NUM_BRANCHES&#39;]): if self.transition3[i] is not None: if i &lt; self.stage3_cfg[&#39;NUM_BRANCHES&#39;]: x_list.append(self.transition3[i](y_list[i])) else: x_list.append(self.transition3[i](y_list[-1])) else: x_list.append(y_list[i]) x = self.stage4(x_list) # Upsampling x0_h, x0_w = x[0].size(2), x[0].size(3) x1 = F.interpolate(x[1], size=(x0_h, x0_w), mode=&#39;bilinear&#39;, align_corners=ALIGN_CORNERS) x2 = F.interpolate(x[2], size=(x0_h, x0_w), mode=&#39;bilinear&#39;, align_corners=ALIGN_CORNERS) x3 = F.interpolate(x[3], size=(x0_h, x0_w), mode=&#39;bilinear&#39;, align_corners=ALIGN_CORNERS) feats = torch.cat([x[0], x1, x2, x3], 1) out_aux_seg = [] # ocr out_aux = self.aux_head(feats) # compute contrast feature feats = self.conv3x3_ocr(feats) context = self.ocr_gather_head(feats, out_aux) feats = self.ocr_distri_head(feats, context) out = self.cls_head(feats) out_aux_seg.append(out_aux) out_aux_seg.append(out) return out_aux_seg def init_weights(self, pretrained=&#39;&#39;,): logger.info(&#39;=&gt; init weights from normal distribution&#39;) for name, m in self.named_modules(): if any(part in name for part in {&#39;cls&#39;, &#39;aux&#39;, &#39;ocr&#39;}): # print(&#39;skipped&#39;, name) continue if isinstance(m, nn.Conv2d): nn.init.normal_(m.weight, std=0.001) elif isinstance(m, BatchNorm2d_class): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) if os.path.isfile(pretrained): pretrained_dict = torch.load( pretrained, map_location={&#39;cuda:0&#39;: &#39;cpu&#39;}) logger.info(&#39;=&gt; loading pretrained model {}&#39;.format(pretrained)) model_dict = self.state_dict() pretrained_dict = {k.replace(&#39;last_layer&#39;, &#39;aux_head&#39;).replace( &#39;model.&#39;, &#39;&#39;): v for k, v in pretrained_dict.items()} print(set(model_dict) - set(pretrained_dict)) print(set(pretrained_dict) - set(model_dict)) pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys()} # for k, _ in pretrained_dict.items(): # logger.info( # &#39;=&gt; loading {} pretrained model {}&#39;.format(k, pretrained)) model_dict.update(pretrained_dict) self.load_state_dict(model_dict) elif pretrained: raise RuntimeError(&#39;No such file {}&#39;.format(pretrained)) . relu . fuse_layer . Test model . def get_seg_model(cfg, **kwargs): model = HighResolutionNet(cfg, **kwargs) # model.init_weights(cfg.MODEL.PRETRAINED) return model model = get_seg_model(config) model.cuda() x = torch.randn(1, 3, 256, 256).cuda() out = model(x) print(out) . HighResolutionNet( (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (transition1): ModuleList( (0): Sequential( (0): Conv2d(256, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Sequential( (0): Conv2d(256, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) ) ) (stage2): Sequential( (0): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None ) ) (relu): ReLU(inplace=True) ) ) (transition2): ModuleList( (0): None (1): None (2): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) ) ) (stage3): Sequential( (0): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None ) ) (relu): ReLU(inplace=True) ) (1): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None ) ) (relu): ReLU(inplace=True) ) (2): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None ) ) (relu): ReLU(inplace=True) ) (3): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None ) ) (relu): ReLU(inplace=True) ) ) (transition3): ModuleList( (0): None (1): None (2): None (3): Sequential( (0): Sequential( (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) ) ) (stage4): Sequential( (0): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): Sequential( (0): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None (3): Sequential( (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(48, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(96, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): Sequential( (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): None ) ) (relu): ReLU(inplace=True) ) (1): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): Sequential( (0): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None (3): Sequential( (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(48, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(96, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): Sequential( (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): None ) ) (relu): ReLU(inplace=True) ) (2): HighResolutionModule( (branches): ModuleList( (0): Sequential( (0): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): Sequential( (0): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (fuse_layers): ModuleList( (0): ModuleList( (0): None (1): Sequential( (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): Sequential( (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): None (2): Sequential( (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): Sequential( (0): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): None (3): Sequential( (0): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): ModuleList( (0): Sequential( (0): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (2): Sequential( (0): Conv2d(48, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Sequential( (0): Sequential( (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (1): Sequential( (0): Conv2d(96, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (2): Sequential( (0): Sequential( (0): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (3): None ) ) (relu): ReLU(inplace=True) ) ) (conv3x3_ocr): Sequential( (0): Conv2d(720, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) ) (ocr_gather_head): SpatialGather_Module() (ocr_distri_head): SpatialOCR_Module( (object_context_block): ObjectAttentionBlock2D( (pool): MaxPool2d(kernel_size=(1, 1), stride=(1, 1), padding=0, dilation=1, ceil_mode=False) (f_pixel): Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (3): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) ) (f_object): Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (3): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) ) (f_down): Sequential( (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) ) (f_up): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) ) ) (conv_bn_dropout): Sequential( (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() ) (2): Dropout2d(p=0.05, inplace=False) ) ) (cls_head): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1)) (aux_head): Sequential( (0): Conv2d(720, 720, kernel_size=(1, 1), stride=(1, 1)) (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): Conv2d(720, 19, kernel_size=(1, 1), stride=(1, 1)) ) ) . [tensor([[[[ 0.0126, 0.2089, 0.1555, ..., -0.2633, -0.2207, -0.1966], [-0.0301, 0.0580, 0.3048, ..., -0.0795, -0.3500, -0.5949], [-0.0560, 0.4462, 0.6888, ..., -0.0914, -0.1904, -0.5955], ..., [-0.2828, -0.3316, -0.3327, ..., 0.0599, -0.1506, 0.1004], [-0.4286, -0.1721, -0.4987, ..., 0.1643, -0.1669, -0.2276], [-0.4279, -0.4016, -0.3705, ..., -0.2004, -0.3496, -0.6257]], [[ 0.2175, 0.1642, 0.2870, ..., -0.2048, -0.1047, -0.1548], [ 0.1737, 0.1060, 0.1426, ..., -0.0630, -0.2284, -0.1860], [ 0.1276, -0.0596, -0.0157, ..., -0.3222, -0.4208, -0.2417], ..., [ 0.2149, -0.1225, 0.2284, ..., -0.0518, -0.3060, -0.1962], [-0.1730, -0.2476, 0.0653, ..., -0.0104, -0.0105, -0.0807], [-0.3364, -0.3119, -0.2409, ..., -0.0378, 0.0649, 0.0647]], [[-0.5266, -0.3146, -0.0793, ..., -0.0764, 0.0503, 0.1208], [-0.6032, -0.5813, -0.3442, ..., -0.4756, -0.2503, -0.1254], [-0.2866, -0.6633, -0.3280, ..., -0.3490, -0.3198, 0.1013], ..., [-0.2102, -0.1872, -0.3686, ..., 0.2620, 0.3569, 0.5164], [-0.2391, -0.0143, -0.3167, ..., 0.1354, 0.4852, 0.4345], [-0.0980, -0.0999, -0.2332, ..., -0.0388, 0.2441, 0.2310]], ..., [[-0.1700, -0.0286, -0.0042, ..., -0.8255, -0.8084, -0.5332], [ 0.0808, 0.1889, 0.0577, ..., -0.6874, -0.4808, -0.2507], [ 0.1011, 0.1252, -0.0381, ..., -0.4298, -0.4084, -0.1023], ..., [ 0.2143, 0.0511, 0.0697, ..., -0.8343, -0.7336, -0.4068], [ 0.3050, -0.1730, -0.1157, ..., -0.8594, -0.7523, -0.6235], [ 0.1338, -0.2147, -0.3771, ..., -0.7364, -0.7153, -0.7533]], [[-0.6849, -0.3476, -0.0539, ..., -0.2995, -0.1954, -0.1797], [-0.6074, -0.4295, -0.3755, ..., -0.4582, -0.4791, -0.5893], [-0.5042, -0.6442, -0.4120, ..., -0.7364, -0.8572, -0.6877], ..., [ 0.1772, 0.1758, 0.1408, ..., 0.2219, 0.3596, 0.1636], [ 0.0977, 0.2228, 0.0980, ..., -0.2784, 0.1537, 0.0426], [ 0.2082, 0.1112, 0.3127, ..., -0.1324, 0.2484, 0.0712]], [[-0.7520, -0.7920, -0.8023, ..., -0.3344, -0.4756, -0.3786], [-0.5891, -0.5307, -0.6568, ..., -0.4531, -0.4720, -0.4871], [-0.7524, -0.7254, -0.7203, ..., -0.5278, -0.4140, -0.3738], ..., [-0.3742, -0.3159, -0.2781, ..., -0.8083, -0.8444, -1.0273], [-0.3527, -0.2352, -0.1589, ..., -0.6653, -0.9925, -1.2293], [-0.4631, -0.5654, -0.5520, ..., -1.1085, -1.1969, -1.6035]]]], device=&#39;cuda:0&#39;, grad_fn=&lt;CudnnConvolutionBackward&gt;), tensor([[[[-1.1921e+00, -8.7566e-01, -7.3220e-01, ..., -7.0549e-01, -1.0782e+00, 1.6466e-01], [-5.1973e-01, -3.2829e-01, -4.0623e-02, ..., -3.4792e-01, -2.0699e-01, 6.5108e-02], [-5.8162e-01, 4.2323e-03, 1.0330e-01, ..., -7.2836e-03, 5.7834e-02, -4.0695e-02], ..., [-2.0400e-01, -1.0644e-01, -5.4366e-02, ..., -1.6851e-01, -1.5228e-01, 4.7855e-01], [-4.7569e-01, 9.6399e-02, -2.8926e-01, ..., -8.0138e-02, -5.0628e-01, 3.7228e-01], [ 9.3950e-02, -4.9347e-02, -1.0575e-01, ..., -9.6701e-01, -1.0447e+00, -2.0369e-02]], [[ 1.3028e+00, 6.7632e-01, 9.9330e-01, ..., 9.1765e-01, 8.9304e-01, 9.7930e-01], [ 6.8199e-01, 4.9716e-01, 3.4814e-01, ..., 7.7114e-01, 6.6421e-01, 3.5449e-01], [ 9.0915e-01, 1.0235e-01, 1.8720e-02, ..., 4.4257e-01, 3.4798e-01, 2.1372e-01], ..., [ 9.0561e-01, -1.4279e-01, 5.0179e-02, ..., 1.8157e-01, -1.3743e-01, 2.9420e-01], [ 9.3803e-01, -1.0436e-02, 1.4767e-01, ..., 4.9976e-02, 1.0739e-01, 2.6171e-01], [ 9.2503e-01, 3.5870e-01, 4.8831e-01, ..., 8.4631e-01, 3.8659e-01, 3.3774e-01]], [[ 2.0981e-01, 6.1833e-01, 9.6794e-01, ..., 2.8238e-01, 2.5978e-01, 4.8613e-01], [-1.2846e-01, 6.7296e-02, -6.3639e-02, ..., -4.2966e-01, 3.1935e-01, 5.5187e-01], [ 2.5797e-01, -1.2030e-01, -2.0252e-01, ..., 1.9528e-01, 1.5962e-01, 3.3269e-01], ..., [-2.0601e-02, 8.2864e-02, -3.6000e-01, ..., 3.5094e-01, -5.5201e-02, -2.5831e-02], [-3.8858e-01, -4.6351e-02, -2.8292e-01, ..., 3.5836e-02, -2.4179e-01, -1.3999e-03], [-8.9333e-02, 4.3436e-01, 4.7767e-01, ..., 9.1226e-01, 7.1973e-01, 5.8763e-01]], ..., [[ 8.5679e-01, 1.1779e-01, 3.7987e-01, ..., 3.8945e-01, 5.4756e-01, 5.8113e-01], [ 2.1517e-01, 2.9140e-01, 8.3521e-02, ..., 4.1642e-01, 4.8541e-02, 5.7063e-01], [ 4.2755e-01, 8.5772e-02, 4.9619e-02, ..., 1.8989e-01, 1.8816e-03, 4.7896e-01], ..., [ 3.2219e-02, -3.5875e-02, 2.6908e-01, ..., 2.4972e-01, 2.1703e-01, 8.5617e-01], [ 3.2784e-01, -6.2063e-02, 1.2819e-01, ..., 1.4086e-01, 2.0888e-01, 1.0915e+00], [ 2.8844e-01, 5.4034e-02, 1.2540e-02, ..., 1.4383e+00, 1.2895e+00, 1.3019e+00]], [[ 3.3014e-01, 2.7361e-01, -6.8319e-01, ..., 1.3882e-01, 9.3643e-02, 1.0667e-02], [ 2.2844e-01, -1.8923e-01, -3.9863e-01, ..., -5.1912e-01, -7.6968e-01, -4.6036e-01], [ 6.4837e-02, -1.6802e-01, -1.0226e-01, ..., -7.3331e-01, -7.6095e-01, -6.2935e-01], ..., [ 2.4629e-01, -3.2840e-01, 6.8417e-02, ..., -2.9780e-01, -2.2339e-01, -6.2761e-01], [-2.2035e-01, -2.5319e-01, 7.7725e-03, ..., -1.8956e-01, -4.1416e-01, -6.3720e-01], [-3.2191e-01, -2.5960e-01, -4.0099e-01, ..., -5.5858e-01, -6.1042e-01, -3.6042e-01]], [[-2.0594e-01, 3.6297e-01, 7.4933e-01, ..., 6.1509e-01, 6.3861e-01, 9.7792e-02], [ 7.0288e-01, 1.9002e-01, 2.3223e-02, ..., 1.9716e-01, 2.1533e-01, 1.1496e-01], [ 5.7000e-01, 2.8020e-01, -5.3618e-02, ..., 1.0633e-01, 3.1564e-01, 2.8380e-01], ..., [ 5.9550e-01, 1.5407e-01, 3.3551e-01, ..., -1.9185e-02, 1.3940e-01, -5.7018e-01], [ 4.7579e-01, 2.6538e-01, 4.5185e-01, ..., 1.8412e-01, 2.5192e-01, -2.6754e-01], [ 7.6439e-01, -1.3286e-01, -4.0669e-01, ..., -8.3105e-01, -6.9897e-01, 2.7346e-01]]]], device=&#39;cuda:0&#39;, grad_fn=&lt;CudnnConvolutionBackward&gt;)] .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai/2020/08/03/Understanding-Hrnet-with-code.html",
            "relUrl": "/pytorch/fastai/2020/08/03/Understanding-Hrnet-with-code.html",
            "date": " • Aug 3, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "pytorch unfold：extract patches from image",
            "content": ". pytorch unfold &amp; fold . Using pytorch unfold and fold to construct the sliding window manually . import torch import numpy as np import matplotlib.pyplot as plt from pathlib import Path from PIL import Image from skimage import io import PIL import os import mimetypes import torchvision.transforms as transforms import glob from skimage.io import imread from natsort import natsorted import re import numba from fastai2.vision.all import * from torchvision.utils import save_image from torchvision.transforms import ToPILImage . from pdb import set_trace . tensor.unfold . x = torch.arange(48).view(3, 4, 4) x.shape # x.view(8,8) x print(&#39;test&#39;) x.unfold(0, 2, 1).shape x.unfold(0, 2, 1) print(&#39;exp1&#39;) x.unfold(0, 3, 3).shape x.unfold(0, 3, 3) print(&#39;exp2&#39;) x.unfold(0, 3, 3).unfold(1, 2, 2).shape x.unfold(0, 3, 3).unfold(1, 2, 2) print(&#39;exp3&#39;) x.unfold(0, 3, 3).unfold(1, 2, 2).unfold(2, 2, 2).shape x.unfold(0, 3, 3).unfold(1, 2, 2).unfold(2, 2, 2) . torch.Size([3, 4, 4]) . tensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]], [[32, 33, 34, 35], [36, 37, 38, 39], [40, 41, 42, 43], [44, 45, 46, 47]]]) . test . torch.Size([2, 4, 4, 2]) . tensor([[[[ 0, 16], [ 1, 17], [ 2, 18], [ 3, 19]], [[ 4, 20], [ 5, 21], [ 6, 22], [ 7, 23]], [[ 8, 24], [ 9, 25], [10, 26], [11, 27]], [[12, 28], [13, 29], [14, 30], [15, 31]]], [[[16, 32], [17, 33], [18, 34], [19, 35]], [[20, 36], [21, 37], [22, 38], [23, 39]], [[24, 40], [25, 41], [26, 42], [27, 43]], [[28, 44], [29, 45], [30, 46], [31, 47]]]]) . exp1 . torch.Size([1, 4, 4, 3]) . tensor([[[[ 0, 16, 32], [ 1, 17, 33], [ 2, 18, 34], [ 3, 19, 35]], [[ 4, 20, 36], [ 5, 21, 37], [ 6, 22, 38], [ 7, 23, 39]], [[ 8, 24, 40], [ 9, 25, 41], [10, 26, 42], [11, 27, 43]], [[12, 28, 44], [13, 29, 45], [14, 30, 46], [15, 31, 47]]]]) . exp2 . torch.Size([1, 2, 4, 3, 2]) . tensor([[[[[ 0, 4], [16, 20], [32, 36]], [[ 1, 5], [17, 21], [33, 37]], [[ 2, 6], [18, 22], [34, 38]], [[ 3, 7], [19, 23], [35, 39]]], [[[ 8, 12], [24, 28], [40, 44]], [[ 9, 13], [25, 29], [41, 45]], [[10, 14], [26, 30], [42, 46]], [[11, 15], [27, 31], [43, 47]]]]]) . exp3 . torch.Size([1, 2, 2, 3, 2, 2]) . tensor([[[[[[ 0, 1], [ 4, 5]], [[16, 17], [20, 21]], [[32, 33], [36, 37]]], [[[ 2, 3], [ 6, 7]], [[18, 19], [22, 23]], [[34, 35], [38, 39]]]], [[[[ 8, 9], [12, 13]], [[24, 25], [28, 29]], [[40, 41], [44, 45]]], [[[10, 11], [14, 15]], [[26, 27], [30, 31]], [[42, 43], [46, 47]]]]]]) . . temp = torch.randint(0, 10, (3, 5176, 3793)) temp.shape patches = temp.unfold(0, 3, 3) patches.shape test_eq(temp.unfold(0, 3, 3), temp.unfold(0, 3, 4)) patches = patches.unfold(1, 128, 128) patches.shape patches = patches.unfold(2, 128, 128) # test_eq(temp.unfold(0,3,3),temp.unfold(0,3,66)) patches.shape . torch.Size([3, 5176, 3793]) . torch.Size([1, 5176, 3793, 3]) . torch.Size([1, 40, 3793, 3, 128]) . torch.Size([1, 40, 29, 3, 128, 128]) . math.floor((5176-128)/128)+1 . 40 . math.floor((3793-128)/128)+1 . 29 . tensor.unfold.rules . important eg. (a,b) = x.shape x.unfold(c,d,e) where d is the size and e is the step from here we can see it:the shape value at dimension c after unfold method is that: eg. at a &#39;s dimension: **(math.floor(a-d)/e +1,b,d)** BTW: the last one is to append the size value in the unfold method . torch.nn.unfold and fold . unfold https://pytorch.org/docs/master/generated/torch.nn.Unfold.html#torch.nn.Unfold . inp = torch.randn(1,3,10,12) w = torch.randn(2,3,4,5) inp_unf = torch.nn.functional.unfold(inp,(4,5)) inp_unf.shape . torch.Size([1, 60, 56]) . fold https://pytorch.org/docs/master/generated/torch.nn.Fold.html?highlight=fold#torch.nn.Fold . experiment on an Image . # !wget https://eoimages.gsfc.nasa.gov/images/imagerecords/88000/88094/niobrara_photo_lrg.jpg patch_size=512 stride=patch_size pil2tensor = transforms.ToTensor() file=Path(&#39;niobrara_photo_lrg.jpg&#39;) filename=file.stem im1 = Image.open(file) print(im1.shape) # im1.resize(5120,5120) im1 = im1.resize((1500,1500),Image.BILINEAR) im1 rgb_image = pil2tensor(im1) rgb_image.shape . (1536, 2048) . torch.Size([3, 1500, 1500]) . rgb_image.data.type() . &#39;torch.FloatTensor&#39; . tensor.unfold . patches = rgb_image.data.unfold(0, 3, 3).unfold(1, patch_size, stride).unfold(2, patch_size, stride) print(patches.shape) . torch.Size([1, 2, 2, 3, 512, 512]) . https://pytorch.org/docs/master/generated/torch.split.html . a = list(patches.shape) . a torch.from_numpy(np.arange(0,a[1])) patches[:,torch.from_numpy(np.arange(0,a[1])),:,:,:,:].shape x = patches[:,torch.from_numpy(np.arange(0,a[1])),:,:,:,:].split(1, dim=1) x = patches.split(1, dim=1) # x = patches.split(1, dim=2) len(x) x[0].shape x[1].shape . [1, 2, 2, 3, 512, 512] . tensor([0, 1]) . torch.Size([1, 2, 2, 3, 512, 512]) . 2 . torch.Size([1, 1, 2, 3, 512, 512]) . torch.Size([1, 1, 2, 3, 512, 512]) . to_pil = ToPILImage() math.floor(1500/512) . 2 . 6000/512 . 11.71875 . x = patches[:,torch.from_numpy(np.arange(0,a[1])),:,:,:,:].split(1, dim=1) for i in list(np.arange(a[1])): y = x[i][:,:,torch.from_numpy(np.arange(0,a[2])),:,:,:].split(1, dim=2) for j in list(np.arange(a[2])): img = to_pil(y[j].squeeze(0).squeeze(0).squeeze(0)) img # set_trace() # save_image(y[j], filename+&#39;-&#39;+str(i)+&#39;-&#39;+str(j)+&#39;.png&#39;) . nn.functional.unfold and fold to extract and reconstruct . https://discuss.pytorch.org/t/seemlessly-blending-tensors-together/65235/9 . def split_tensor(tensor, tile_size=256): mask = torch.ones_like(tensor) # use torch.nn.Unfold stride = tile_size//2 unfold = nn.Unfold(kernel_size=(tile_size, tile_size), stride=stride) # Apply to mask and original image mask_p = unfold(mask) patches = unfold(tensor) patches = patches.reshape(3, tile_size, tile_size, -1).permute(3, 0, 1, 2) if tensor.is_cuda: patches_base = torch.zeros(patches.size(), device=tensor.get_device()) else: patches_base = torch.zeros(patches.size()) tiles = [] for t in range(patches.size(0)): tiles.append(patches[[t], :, :, :]) return tiles, mask_p, patches_base, (tensor.size(2), tensor.size(3)) def rebuild_tensor(tensor_list, mask_t, base_tensor, t_size, tile_size=256): stride = tile_size//2 # base_tensor here is used as a container for t, tile in enumerate(tensor_list): print(tile.size()) base_tensor[[t], :, :] = tile base_tensor = base_tensor.permute(1, 2, 3, 0).reshape(3*tile_size*tile_size, base_tensor.size(0)).unsqueeze(0) fold = nn.Fold(output_size=(t_size[0], t_size[1]), kernel_size=(tile_size, tile_size), stride=stride) # https://discuss.pytorch.org/t/seemlessly-blending-tensors-together/65235/2?u=bowenroom output_tensor = fold(base_tensor)/fold(mask_t) # output_tensor = fold(base_tensor) return output_tensor . # %%time test_image = &#39;test_image.jpg&#39; image_size=1024 Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()]) input_tensor = Loader(Image.open(file).convert(&#39;RGB&#39;)).unsqueeze(0).cuda() # Split image into overlapping tiles tile_tensors, mask_t, base_tensor, t_size = split_tensor(input_tensor, 660) # Put tiles back together output_tensor = rebuild_tensor(tile_tensors, mask_t, base_tensor, t_size, 660) # Save Output Image2PIL = transforms.ToPILImage() print(f&#39;the whole length of the patches is {len(tile_tensors)}&#39;) # show small patches for i in range(len(tile_tensors)): print(f&#39;the current is {i}&#39;) Image2PIL(tile_tensors[i].cpu().squeeze(0)) print(&#39;the reconstruct image&#39;) Image2PIL(output_tensor.cpu().squeeze(0)) # Image2PIL(output_tensor.cpu().squeeze(0)).save(&#39;output_image.png&#39;) . torch.Size([1, 3, 660, 660]) torch.Size([1, 3, 660, 660]) torch.Size([1, 3, 660, 660]) torch.Size([1, 3, 660, 660]) torch.Size([1, 3, 660, 660]) torch.Size([1, 3, 660, 660]) the whole length of the patches is 6 the current is 0 . the current is 1 . the current is 2 . the current is 3 . the current is 4 . the current is 5 . the reconstruct image . 6000/512 . 11.71875 . fastai2.PILImage and PIL.image . len(tile_tensors) tile_tensors[0].size() . 6 . torch.Size([1, 3, 660, 660]) . tile_tensors[0].squeeze(0) . tensor([[[1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], ..., [0.5647, 0.5451, 0.4902, ..., 0.7490, 0.7569, 0.7412], [0.5490, 0.5529, 0.4863, ..., 0.7412, 0.7490, 0.7412], [0.5608, 0.5686, 0.5059, ..., 0.7412, 0.7569, 0.7529]], [[1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], ..., [0.6667, 0.6549, 0.6196, ..., 0.7647, 0.7686, 0.7490], [0.6431, 0.6510, 0.6039, ..., 0.7451, 0.7529, 0.7412], [0.6471, 0.6549, 0.6078, ..., 0.7333, 0.7451, 0.7412]], [[1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], [1.0000, 1.0000, 1.0000, ..., 1.0000, 1.0000, 1.0000], ..., [0.3765, 0.3647, 0.3176, ..., 0.8000, 0.8039, 0.7882], [0.3412, 0.3569, 0.3098, ..., 0.7843, 0.7882, 0.7804], [0.3373, 0.3608, 0.3255, ..., 0.7765, 0.7882, 0.7843]]], device=&#39;cuda:0&#39;) . temp = PILImage(Image2PIL(tile_tensors[0].cpu().squeeze(0))) temp temp.shape . (660, 660) .",
            "url": "https://bowenroom.github.io/myBlog/pytorch/fastai2/2020/06/01/torch-unfold.html",
            "relUrl": "/pytorch/fastai2/2020/06/01/torch-unfold.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bowenroom.github.io/myBlog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bowenroom.github.io/myBlog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bowenroom.github.io/myBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bowenroom.github.io/myBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}