<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Transformer Blog | Bowenroom</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Transformer Blog" />
<meta name="author" content="Bowen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="learn DL easily" />
<meta property="og:description" content="learn DL easily" />
<link rel="canonical" href="https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html" />
<meta property="og:url" content="https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html" />
<meta property="og:site_name" content="Bowenroom" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-10T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Bowen"},"description":"learn DL easily","@type":"BlogPosting","headline":"Transformer Blog","dateModified":"2021-01-10T00:00:00-06:00","datePublished":"2021-01-10T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html"},"url":"https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/myBlog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bowenroom.github.io/myBlog/feed.xml" title="Bowenroom" /><link rel="shortcut icon" type="image/x-icon" href="/myBlog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Transformer Blog | Bowenroom</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Transformer Blog" />
<meta name="author" content="Bowen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="learn DL easily" />
<meta property="og:description" content="learn DL easily" />
<link rel="canonical" href="https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html" />
<meta property="og:url" content="https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html" />
<meta property="og:site_name" content="Bowenroom" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-10T00:00:00-06:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Bowen"},"description":"learn DL easily","@type":"BlogPosting","headline":"Transformer Blog","dateModified":"2021-01-10T00:00:00-06:00","datePublished":"2021-01-10T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html"},"url":"https://bowenroom.github.io/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://bowenroom.github.io/myBlog/feed.xml" title="Bowenroom" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/myBlog/">Bowenroom</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/myBlog/about/">About Me</a><a class="page-link" href="/myBlog/search/">Search</a><a class="page-link" href="/myBlog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformer Blog</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-10T00:00:00-06:00" itemprop="datePublished">
        Jan 10, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Bowen</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/myBlog/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/myBlog/categories/#fastai">fastai</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/bowenroom/myBlog/tree/master/_notebooks/2021-01-10-transformer-blog.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/myBlog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/bowenroom/myBlog/master?filepath=_notebooks%2F2021-01-10-transformer-blog.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/myBlog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/bowenroom/myBlog/blob/master/_notebooks/2021-01-10-transformer-blog.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/myBlog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#self-attention-method">self-attention method </a></li>
<li class="toc-entry toc-h2"><a href="#multi-head-attention">multi-head attention </a></li>
<li class="toc-entry toc-h2"><a href="#transformer-structure">transformer structure </a></li>
<li class="toc-entry toc-h2"><a href="#position-embedding">position embedding </a></li>
<li class="toc-entry toc-h2"><a href="#position-encoding">position encoding </a></li>
<li class="toc-entry toc-h2"><a href="#Transformer-structure">Transformer structure </a></li>
<li class="toc-entry toc-h2"><a href="#mask">mask </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-10-transformer-blog.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>reference links which helps a lot:</p>
<ul>
<li>[1] <a href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a>
</li>
<li>[2] <a href="http://peterbloem.nl/blog/transformers">http://peterbloem.nl/blog/transformers</a>
</li>
<li>[3] <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="self-attention-method">
<a class="anchor" href="#self-attention-method" aria-hidden="true"><span class="octicon octicon-link"></span></a>self-attention method<a class="anchor-link" href="#self-attention-method"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://i.loli.net/2021/01/11/D2rtu9nGSVbdAUq.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://i.loli.net/2021/01/11/VW1UubNORzTy7E6.png" alt=""></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">raw_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">raw_weights</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">weights</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[3.7608e-01, 1.4019e-01, 4.8374e-01],
         [1.3241e-02, 9.8102e-01, 5.7356e-03],
         [6.7872e-02, 8.5199e-03, 9.2361e-01]],

        [[8.6212e-01, 1.9809e-02, 1.1807e-01],
         [6.0491e-04, 9.8575e-01, 1.3647e-02],
         [1.1610e-01, 4.3942e-01, 4.4448e-01]]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[ 0.7534,  0.1771, -0.0526, -0.4511],
         [ 0.0585, -1.1643,  1.3854,  0.4464],
         [ 1.2452,  0.3228, -0.4401, -0.9620]],

        [[-0.8668,  0.0406,  0.0529, -0.2900],
         [ 1.9601, -0.1198, -0.9540,  0.3403],
         [ 0.9987, -0.2199, -0.2520, -0.0090]]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">()</span>
<span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">temp</span>
<span class="n">yy</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">yy</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>4912394464713312614</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[1, 1],
        [0, 1]])</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.7311, 0.5000],
        [0.2689, 0.5000]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://i.loli.net/2021/01/11/IshpEO5UHlXvw8k.png" alt=""></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">aa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">aa</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[2, 9, 7, 7],
       [2, 3, 3, 4],
       [2, 2, 8, 9]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">aa</span><span class="p">[:,</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[2, 7],
       [2, 3],
       [2, 8]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="multi-head-attention">
<a class="anchor" href="#multi-head-attention" aria-hidden="true"><span class="octicon octicon-link"></span></a>multi-head attention<a class="anchor-link" href="#multi-head-attention"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SelfAttentionWide</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param emb:</span>
<span class="sd">        :param heads:</span>
<span class="sd">        :param mask:</span>
<span class="sd">        """</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">heads</span> <span class="o">*</span> <span class="n">emb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="k">assert</span> <span class="n">e</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">,</span> <span class="sa">f</span><span class="s1">'Input embedding dim (</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s1">) should match layer embedding dim (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="si">}</span><span class="s1">)'</span>

        <span class="n">keys</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">values</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

        <span class="c1"># compute scaled dot-product self-attention</span>

        <span class="c1"># - fold heads into the batch dimension</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">/</span> <span class="p">(</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="n">keys</span>    <span class="o">=</span> <span class="n">keys</span> <span class="o">/</span> <span class="p">(</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="c1"># - Instead of dividing the dot products by sqrt(e), we scale the keys and values.</span>
        <span class="c1">#   This should be more memory efficient</span>

        <span class="c1"># - get dot product of queries and keys, and scale</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">assert</span> <span class="n">dot</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">:</span> <span class="c1"># mask out the upper half of the dot matrix, excluding the diagonal</span>
            <span class="n">mask_</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">maskval</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">'-inf'</span><span class="p">),</span> <span class="n">mask_diagonal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># - dot now has row-wise self-attention probabilities</span>

        <span class="c1"># apply the self attention to the values</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

        <span class="c1"># swap h, t back, unify heads</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span> <span class="o">*</span> <span class="n">e</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SelfAttentionNarrow</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param emb:</span>
<span class="sd">        :param heads:</span>
<span class="sd">        :param mask:</span>
<span class="sd">        """</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">assert</span> <span class="n">emb</span> <span class="o">%</span> <span class="n">heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sa">f</span><span class="s1">'Embedding dimension (</span><span class="si">{</span><span class="n">emb</span><span class="si">}</span><span class="s1">) should be divisible by nr. of heads (</span><span class="si">{</span><span class="n">heads</span><span class="si">}</span><span class="s1">)'</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">emb</span> <span class="o">//</span> <span class="n">heads</span>
        <span class="c1"># - We will break the embedding into `heads` chunks and feed each to a different attention head</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span>  <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">heads</span> <span class="o">*</span> <span class="n">s</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="k">assert</span> <span class="n">e</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">,</span> <span class="sa">f</span><span class="s1">'Input embedding dim (</span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s1">) should match layer embedding dim (</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="si">}</span><span class="s1">)'</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">e</span> <span class="o">//</span> <span class="n">h</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="n">keys</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">keys</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">queries</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">values</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="c1"># Compute scaled dot-product self-attention</span>

        <span class="c1"># - fold heads into the batch dimension</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">/</span> <span class="p">(</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="n">keys</span>    <span class="o">=</span> <span class="n">keys</span> <span class="o">/</span> <span class="p">(</span><span class="n">e</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">4</span><span class="p">))</span>
        <span class="c1"># - Instead of dividing the dot products by sqrt(e), we scale the keys and values.</span>
        <span class="c1">#   This should be more memory efficient</span>

        <span class="c1"># - get dot product of queries and keys, and scale</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

        <span class="k">assert</span> <span class="n">dot</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="p">(</span><span class="n">b</span><span class="o">*</span><span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="p">:</span> <span class="c1"># mask out the upper half of the dot matrix, excluding the diagonal</span>
            <span class="n">mask_</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">maskval</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">'-inf'</span><span class="p">),</span> <span class="n">mask_diagonal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># - dot now has row-wise self-attention probabilities</span>

        <span class="c1"># apply the self attention to the values</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

        <span class="c1"># swap h, t back, unify heads</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>we can also change the code using einsum which help to short the code part and have a nice execution time <a href="https://rockt.github.io/2018/04/30/einsum">https://rockt.github.io/2018/04/30/einsum</a></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SelfAttentionWideEinsum</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param emb:</span>
<span class="sd">        :param heads:</span>
<span class="sd">        :param mask:</span>
<span class="sd">        """</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">emb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">emb</span> <span class="o">*</span> <span class="n">heads</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">heads</span> <span class="o">*</span> <span class="n">emb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward_einsum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>

        <span class="n">keys</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokeys</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toqueries</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
        <span class="n">values</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tovalues</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>

        <span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'bthe,bihe-&gt;bhti'</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'bhtd,bdhe-&gt;bthe'</span><span class="p">,</span> <span class="n">dot</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>

        <span class="c1"># we can move reshape of weights to init; I left it here just to compare with the original implementation</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">'bthe,khe-&gt;btk'</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">e</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">e</span><span class="p">))</span> 
        <span class="k">return</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">unifyheads</span><span class="o">.</span><span class="n">bias</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="transformer-structure">
<a class="anchor" href="#transformer-structure" aria-hidden="true"><span class="octicon octicon-link"></span></a>transformer structure<a class="anchor-link" href="#transformer-structure"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://i.loli.net/2021/01/11/ri7wu9jPdohARF1.png" alt=""></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">ff_hidden_mult</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">wide</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">SelfAttentionWide</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span> <span class="k">if</span> <span class="n">wide</span> \
                    <span class="k">else</span> <span class="n">SelfAttentionNarrow</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
<span class="sd">'''</span>
<span class="sd">We’ve made the relatively arbitrary choice of making the hidden layer</span>
<span class="sd">of the feedforward 4 times as big as the input and output. Smaller values may work as well, </span>
<span class="sd">and save memory, but it should be bigger than the input/output layers.</span>
<span class="sd">'''</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">emb</span><span class="p">,</span> <span class="n">ff_hidden_mult</span> <span class="o">*</span> <span class="n">emb</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">ff_hidden_mult</span> <span class="o">*</span> <span class="n">emb</span><span class="p">,</span> <span class="n">emb</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">do</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>

        <span class="n">attended</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">attended</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">do</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">fedforward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">fedforward</span> <span class="o">+</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">do</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="position-embedding">
<a class="anchor" href="#position-embedding" aria-hidden="true"><span class="octicon octicon-link"></span></a>position embedding<a class="anchor-link" href="#position-embedding"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">d</span><span class="p">(</span><span class="n">tensor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Returns a device string either for the best available device,</span>
<span class="sd">    or for the device corresponding to the argument</span>
<span class="sd">    :param tensor:</span>
<span class="sd">    :return:</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">tensor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">'cpu'</span>
    <span class="k">return</span> <span class="s1">'cuda'</span> <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">is_cuda</span> <span class="k">else</span> <span class="s1">'cpu'</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">temp</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-0.9651,  1.4951, -0.0168,  0.7085],
         [-1.9934, -0.5921, -0.3682, -0.8308],
         [ 1.0251, -0.0033, -1.4288,  0.4307]],

        [[-0.1145,  0.4717, -0.5771,  0.8367],
         [ 0.8415, -0.2907,  2.7137, -0.3131],
         [ 1.2084,  0.0839, -0.4571, -0.1604]]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="c1"># an Embedding module containing 10 tensors of size 6</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># a batch of 2 samples of 4 indices each</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-1.0416, -1.2013, -1.1024, -0.2295,  0.7987,  0.5698],
         [-0.9966,  0.5302, -0.6908, -2.4040, -0.1549, -0.0050],
         [ 0.1405, -0.4664, -0.2933, -0.0160,  0.0548, -0.3741]]],
       grad_fn=&lt;EmbeddingBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">temp</span><span class="p">[</span><span class="kc">None</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([1, 2, 3, 4])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">b</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="n">e</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span>
<span class="n">position</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">)(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">t</span><span class="p">))[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
<span class="n">position</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[-1.6559, -0.3209,  2.2730,  0.3641, -1.5789, -1.0718],
         [-0.3217,  0.2345,  1.8767, -0.8459, -1.0136,  0.1944],
         [ 0.2643, -1.5120, -0.1799,  1.8587,  0.7489,  0.0663],
         [-0.2499,  0.6199,  0.6119, -0.1948, -1.2249, -0.9786],
         [-0.0888,  1.4573, -0.0139, -1.5792,  1.0114, -0.6898]],

        [[-1.6559, -0.3209,  2.2730,  0.3641, -1.5789, -1.0718],
         [-0.3217,  0.2345,  1.8767, -0.8459, -1.0136,  0.1944],
         [ 0.2643, -1.5120, -0.1799,  1.8587,  0.7489,  0.0663],
         [-0.2499,  0.6199,  0.6119, -0.1948, -1.2249, -0.9786],
         [-0.0888,  1.4573, -0.0139, -1.5792,  1.0114, -0.6898]],

        [[-1.6559, -0.3209,  2.2730,  0.3641, -1.5789, -1.0718],
         [-0.3217,  0.2345,  1.8767, -0.8459, -1.0136,  0.1944],
         [ 0.2643, -1.5120, -0.1799,  1.8587,  0.7489,  0.0663],
         [-0.2499,  0.6199,  0.6119, -0.1948, -1.2249, -0.9786],
         [-0.0888,  1.4573, -0.0139, -1.5792,  1.0114, -0.6898]]],
       grad_fn=&lt;ExpandBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="position-encoding">
<a class="anchor" href="#position-encoding" aria-hidden="true"><span class="octicon octicon-link"></span></a>position encoding<a class="anchor-link" href="#position-encoding"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Code from https://www.tensorflow.org/tutorials/text/transformer</span>
<span class="k">def</span> <span class="nf">get_angles</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rates</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">pos</span> <span class="o">*</span> <span class="n">angle_rates</span>

<span class="k">def</span> <span class="nf">positional_encoding</span><span class="p">(</span><span class="n">position</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
  <span class="n">angle_rads</span> <span class="o">=</span> <span class="n">get_angles</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">position</span><span class="p">)[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span>
                          <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">d_model</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span>
                          <span class="n">d_model</span><span class="p">)</span>
  
  <span class="c1"># apply sin to even indices in the array; 2i</span>
  <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
  
  <span class="c1"># apply cos to odd indices in the array; 2i+1</span>
  <span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_rads</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
    
  <span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">angle_rads</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span>
    
  <span class="k">return</span> <span class="n">pos_encoding</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">dimensions</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">positional_encoding</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">pos_encoding</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">pos_encoding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'viridis'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Embedding Dimensions'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">tokens</span><span class="p">,</span><span class="mi">0</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Token Position'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(1, 10, 64)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;Figure size 864x576 with 0 Axes&gt;</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.collections.QuadMesh at 0x7f943d17ac50&gt;</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0.5, 0, 'Embedding Dimensions')</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(0.0, 64.0)</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(10.0, 0.0)</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Text(0, 0.5, 'Token Position')</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.colorbar.Colorbar at 0x7f943d12c5c0&gt;</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-structure">
<a class="anchor" href="#Transformer-structure" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer structure<a class="anchor-link" href="#Transformer-structure"> </a>
</h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_tokens</span> <span class="o">=</span> <span class="n">num_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

		<span class="c1"># The sequence of transformer blocks that does all the </span>
		<span class="c1"># heavy lifting</span>
        <span class="n">tblocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="p">):</span>
            <span class="n">tblocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="n">heads</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tblocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">tblocks</span><span class="p">)</span>

		<span class="c1"># Maps the final output sequence to class logits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">toprobs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">"""</span>
<span class="sd">        :param x: A (b, t) tensor of integer values representing </span>
<span class="sd">                  words (in some predetermined vocabulary).</span>
<span class="sd">        :return: A (b, c) tensor of log-probabilities over the </span>
<span class="sd">                 classes (where c is the nr. of classes).</span>
<span class="sd">        """</span>
		<span class="c1"># generate token embeddings</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

		<span class="c1"># generate position embeddings</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="n">positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">positions</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        
        <span class="n">x</span> <span class="o">=</span> <span class="n">tokens</span> <span class="o">+</span> <span class="n">positions</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tblocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># Average-pool over the t dimension and project to class </span>
        <span class="c1"># probabilities</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toprobs</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="mask">
<a class="anchor" href="#mask" aria-hidden="true"><span class="octicon octicon-link"></span></a>mask<a class="anchor-link" href="#mask"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="https://i.loli.net/2021/01/12/PshiNdajJzu6oOR.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>
<a href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu">https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu</a>
<a href="https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices">https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices</a>
</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">iu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">iu2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">iu1</span>
<span class="c1"># iu2</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(array([0, 0, 0, 0, 1, 1, 1, 2, 2, 3]), array([0, 1, 2, 3, 1, 2, 3, 2, 3, 3]))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a</span><span class="p">[</span><span class="n">iu1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([ 0,  1,  2,  3,  5,  6,  7, 10, 11, 15])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">[</span><span class="n">iu2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([2, 3, 7])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">iu1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="n">a</span> 
<span class="n">iu1</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]])</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0, 0, 0, 0, 1, 1, 1, 2, 2, 3],
        [0, 1, 2, 3, 1, 2, 3, 2, 3, 3]])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span><span class="p">[</span><span class="n">iu1</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">iu1</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 0,  1,  2,  3,  5,  6,  7, 10, 11, 15])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># mask function</span>
<span class="k">def</span> <span class="nf">mask_</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">maskval</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">mask_diagonal</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Masks out all values in the given batch of matrices where i &lt;= j holds,</span>
<span class="sd">    i &lt; j if mask_diagonal is false</span>
<span class="sd">    In place operation</span>
<span class="sd">    :param tns:</span>
<span class="sd">    :return:</span>
<span class="sd">    """</span>

    <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">matrices</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span> <span class="k">if</span> <span class="n">mask_diagonal</span> <span class="k">else</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">matrices</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">maskval</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">queries</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">dot</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">'-inf'</span><span class="p">)</span>

<span class="n">dot</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dot</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> 
<span class="n">indices</span>
<span class="n">dot</span>
<span class="n">dot</span><span class="p">[:,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0, 0, 1],
        [1, 2, 2]])</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[[1.0000e+00, 0.0000e+00, 0.0000e+00],
         [9.0848e-01, 9.1524e-02, 0.0000e+00],
         [2.5938e-04, 9.9937e-01, 3.6845e-04]]])</pre>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0., 0., 0.]])</pre>
</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="bowenroom/myBlog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/myBlog/pytorch/fastai/2021/01/10/transformer-blog.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/myBlog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/myBlog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/myBlog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>learn DL easily</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/bowenroom" title="bowenroom"><svg class="svg-icon grey"><use xlink:href="/myBlog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/bowenroom1" title="bowenroom1"><svg class="svg-icon grey"><use xlink:href="/myBlog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
