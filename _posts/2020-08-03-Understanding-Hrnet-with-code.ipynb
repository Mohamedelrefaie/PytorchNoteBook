{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T08:21:44.667132Z",
     "start_time": "2020-08-05T08:21:43.685144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 128, 128])\n",
      "torch.Size([1, 96, 64, 64])\n",
      "tensor([[-1.8285e-01,  3.7024e-02, -2.6178e-01,  2.3370e-01,  3.1028e-01,\n",
      "          3.5811e-01, -2.8304e-02, -3.1331e-02,  2.2155e-02,  5.1546e-02,\n",
      "         -3.0682e-01, -7.1909e-02, -1.8826e-01, -7.2714e-02,  1.1678e-01,\n",
      "          3.4943e-01,  1.0034e-01,  2.4758e-01,  1.4285e-01,  1.5920e-01,\n",
      "          2.1332e-01, -1.2861e-01, -2.8684e-01, -2.1302e-01, -1.1635e-01,\n",
      "          8.4607e-02,  4.4523e-02, -6.8429e-01, -2.2621e-01, -2.3086e-01,\n",
      "         -3.5478e-01,  2.2668e-02,  1.9053e-02, -1.5617e-01, -2.0598e-01,\n",
      "          2.2977e-01, -3.1025e-01, -1.1416e-01, -2.6373e-01,  3.6234e-01,\n",
      "          3.1318e-01,  2.4282e-01, -1.9437e-01,  1.0783e-01, -6.9517e-02,\n",
      "         -2.0512e-01,  2.4396e-01,  2.4557e-01, -2.0485e-01, -1.9056e-02,\n",
      "         -3.9897e-02,  4.4720e-01, -1.8321e-01, -9.6457e-02, -2.7392e-01,\n",
      "         -3.5969e-01, -1.1037e-01, -8.4277e-02, -1.4069e-01, -5.0973e-02,\n",
      "         -1.1162e-01, -2.9342e-01,  4.8016e-02, -4.0399e-02, -1.5885e-01,\n",
      "          5.0039e-01, -1.8905e-01, -2.9307e-01,  1.9272e-01,  1.0305e-01,\n",
      "          2.7582e-01, -2.9067e-01,  5.5229e-02,  1.8179e-01, -4.7120e-01,\n",
      "          7.4054e-02,  5.2533e-02, -5.6464e-03, -1.7592e-01, -5.5958e-02,\n",
      "         -1.5977e-01,  1.3091e-01,  1.1524e-01,  8.3932e-02,  5.3071e-01,\n",
      "          2.3650e-01,  4.0428e-01,  8.7510e-02,  9.9927e-02,  2.6240e-02,\n",
      "          4.2725e-01, -1.8982e-01,  1.8949e-01,  1.0610e-01,  3.3134e-01,\n",
      "          4.6541e-02, -5.1692e-03,  3.1384e-01,  9.6531e-02,  8.2254e-02,\n",
      "         -1.2788e-01,  4.1901e-01, -2.1224e-01,  5.3329e-01, -1.0165e-01,\n",
      "          8.9432e-02,  4.3582e-02, -3.5091e-01,  2.7001e-01,  2.3731e-01,\n",
      "          2.1898e-01, -2.0065e-02,  1.5323e-01,  3.7907e-02, -2.0556e-01,\n",
      "         -1.4314e-02, -2.6556e-01,  1.8389e-01,  3.0863e-02,  1.8049e-01,\n",
      "          3.3321e-01, -2.0882e-01, -1.8299e-01, -1.0702e-01, -3.2443e-02,\n",
      "         -2.0648e-01, -1.4470e-01, -8.8827e-02,  2.8832e-01, -2.8162e-02,\n",
      "         -5.7557e-02,  4.2172e-01,  4.8015e-02,  1.2660e-01,  1.6091e-01,\n",
      "         -2.9161e-01,  6.3323e-02,  6.0202e-02, -1.7645e-01,  1.4941e-01,\n",
      "          4.5459e-04, -4.9636e-01, -1.6886e-01, -1.4257e-01, -1.2880e-01,\n",
      "         -2.7895e-01,  1.4967e-01, -2.1424e-01,  3.3568e-02, -5.6435e-02,\n",
      "          4.0646e-02,  2.7571e-01, -2.0091e-01,  1.9597e-01, -1.0292e-03,\n",
      "          8.3878e-02, -3.4518e-01,  1.5791e-01,  1.5913e-01, -6.9369e-03,\n",
      "          1.1951e-01,  2.1764e-01,  3.9360e-01,  3.2540e-01, -2.4914e-01,\n",
      "          2.2427e-01, -1.6615e-01, -3.0620e-01,  2.3437e-01, -2.1953e-01,\n",
      "          6.5438e-02,  4.7198e-01,  2.0517e-01,  1.2067e-01,  1.8361e-01,\n",
      "          2.3318e-02,  4.6146e-01,  2.8233e-01, -4.3791e-02,  3.6481e-01,\n",
      "         -2.6108e-01, -6.9707e-01, -6.2338e-01, -7.2153e-03,  4.6349e-01,\n",
      "          3.6214e-01,  1.4462e-01, -2.2325e-01,  3.7730e-01,  2.8843e-02,\n",
      "         -8.5308e-02, -1.9946e-02, -3.8079e-01,  1.4398e-01,  7.8440e-02,\n",
      "         -2.6882e-01,  1.6760e-01, -4.5211e-01,  4.9921e-02, -8.6174e-02,\n",
      "          1.0736e-01, -3.1273e-01,  2.7402e-02, -2.9284e-02,  5.7606e-01,\n",
      "          1.1432e-01,  1.4912e-01,  3.0641e-01, -2.3999e-02, -1.3718e-02,\n",
      "         -2.3459e-01,  1.7366e-01, -3.2200e-02, -5.8191e-02,  1.7883e-01,\n",
      "         -1.1510e-01, -3.6270e-01, -2.8240e-01,  1.3691e-01, -1.5962e-01,\n",
      "         -8.5462e-02,  4.9860e-02,  1.5072e-01,  3.8690e-01, -2.8697e-01,\n",
      "          1.1934e-01,  1.0075e-01,  2.6709e-03,  5.2589e-01,  3.1442e-01,\n",
      "         -3.1638e-01, -1.8350e-01, -2.1714e-01,  2.5677e-02,  3.1192e-01,\n",
      "          1.2925e-01,  9.2672e-02,  1.6325e-01, -4.0229e-01, -3.7441e-02,\n",
      "         -1.7556e-01, -8.8458e-02,  5.1389e-02,  3.0635e-01,  1.7256e-01,\n",
      "          8.8009e-02, -7.4204e-02,  8.7419e-03, -2.5200e-01,  3.3823e-01,\n",
      "          2.2534e-01, -2.0375e-01, -4.0460e-01,  3.4761e-01, -1.0738e-01,\n",
      "          4.3112e-01,  4.5839e-02,  3.0461e-01, -1.2437e-02,  1.4098e-01,\n",
      "         -1.6453e-01,  1.3912e-01,  4.8816e-02, -1.9352e-01, -1.7833e-01,\n",
      "          1.8168e-01,  3.2813e-01, -4.9669e-02,  3.3392e-01, -4.4134e-01,\n",
      "         -1.2905e-01, -1.5263e-01,  2.0159e-01, -6.8992e-02,  3.2324e-02,\n",
      "          1.5713e-01, -2.5826e-01, -2.1154e-01,  3.6618e-01, -1.7381e-01,\n",
      "         -4.3557e-02, -1.2592e-01,  1.0160e-02,  1.6162e-01, -1.6375e-01,\n",
      "         -1.0628e-01,  6.2773e-03,  3.5737e-01, -4.0417e-01,  4.7719e-01,\n",
      "          5.6037e-02, -1.9488e-02, -3.0267e-01, -1.6062e-01, -9.9734e-02,\n",
      "         -1.0358e-01,  7.7393e-02,  2.1423e-01,  1.8513e-01,  1.1284e-01,\n",
      "          1.5298e-01, -2.1770e-01,  8.1810e-03,  4.1480e-01,  1.3544e-01,\n",
      "          2.2241e-01,  1.5179e-01, -3.2318e-01, -3.5896e-01,  1.0199e-01,\n",
      "         -1.2304e-02,  2.2051e-01,  1.3885e-01, -2.6801e-01, -2.0999e-02,\n",
      "          3.7965e-02,  1.3689e-01, -1.1809e-01, -2.3062e-02, -2.7554e-02,\n",
      "          3.4797e-01,  1.3897e-01, -2.5152e-01, -4.9363e-01, -1.2160e-01,\n",
      "         -2.4432e-01, -1.5403e-01, -4.2343e-02, -2.6259e-01, -1.3205e-01,\n",
      "         -2.3451e-01,  7.3893e-02, -2.0812e-01, -4.6583e-02,  1.6835e-01,\n",
      "          5.4632e-02, -1.1156e-01,  1.0660e-01, -2.0783e-01, -4.6784e-03,\n",
      "          2.1649e-01, -3.6653e-01,  6.8711e-02,  8.3109e-02,  2.3918e-01,\n",
      "         -1.1470e-01,  1.1449e-01,  6.9768e-03,  2.4450e-01, -2.1697e-01,\n",
      "         -6.2738e-01, -5.4695e-01, -6.8161e-02,  4.6069e-01,  3.7118e-01,\n",
      "         -3.1287e-01,  3.3193e-02, -2.6193e-01, -2.4757e-01,  2.7455e-01,\n",
      "          2.1974e-01,  2.5717e-01,  2.3713e-02,  4.0637e-01, -2.6044e-01,\n",
      "          1.2908e-01, -7.6544e-02, -1.7394e-01, -6.2313e-02,  1.1636e-02,\n",
      "         -2.6676e-01, -1.4071e-01, -2.7307e-01, -3.2461e-02,  2.4004e-01,\n",
      "          4.3061e-01, -5.0204e-02, -1.4709e-01, -3.7643e-01, -2.0584e-01,\n",
      "          3.9586e-01, -1.1202e-01,  1.0582e-02,  1.2343e-01,  4.6482e-01,\n",
      "          6.3071e-02,  1.5777e-01,  1.0410e-01,  3.2164e-02,  1.2327e-01,\n",
      "         -2.5593e-01,  1.6097e-02,  2.6287e-01, -3.9835e-02,  1.2678e-01,\n",
      "         -5.0571e-02, -1.1015e-01, -4.8955e-03, -9.8429e-02,  7.9936e-02,\n",
      "         -4.0669e-01, -1.6793e-01,  2.5316e-01,  2.0111e-01,  3.6479e-01,\n",
      "         -6.4099e-02, -2.7894e-01,  3.0241e-01, -2.9578e-01,  2.0498e-01,\n",
      "         -3.2518e-01,  1.2865e-01, -1.7107e-01, -1.7040e-01, -6.5615e-02,\n",
      "          4.6304e-01,  3.6797e-02,  9.4724e-02,  3.2858e-03,  2.6811e-01,\n",
      "          3.0845e-01, -4.6041e-02,  1.7997e-01,  1.5076e-01,  3.6813e-02,\n",
      "          3.4276e-01, -1.4578e-02,  2.6275e-01, -1.6871e-01,  2.7034e-01,\n",
      "         -6.7389e-02,  3.1108e-01,  2.4397e-01, -2.4707e-01, -3.4267e-01,\n",
      "          9.8712e-02,  2.7897e-01, -1.0152e-01,  4.8815e-01,  1.3545e-02,\n",
      "         -1.2803e-01, -4.1529e-01, -1.1586e-01, -1.8722e-01,  1.6481e-01,\n",
      "         -4.1369e-01, -3.6265e-01, -2.1286e-01,  2.8185e-01,  2.0436e-02,\n",
      "         -2.2785e-01, -1.1140e-01, -2.2937e-02, -8.2849e-02,  1.0387e-01,\n",
      "         -4.7634e-02, -1.1608e-01, -1.1716e-01,  6.0931e-01, -4.3094e-01,\n",
      "          9.8931e-02,  5.7210e-02, -4.3890e-01, -2.2846e-01,  1.1123e-01,\n",
      "          9.2171e-02,  1.9954e-01, -2.0652e-01, -2.0706e-01,  4.2777e-01,\n",
      "         -2.7856e-01, -2.2523e-01, -6.8828e-01, -2.5418e-01, -1.8717e-01,\n",
      "         -5.3982e-02,  2.9737e-02, -2.4293e-01,  3.7357e-02,  9.3341e-02,\n",
      "         -2.6839e-01, -2.1713e-01, -1.1684e-01, -3.8822e-02, -4.7450e-01,\n",
      "          1.0351e-02, -2.4872e-01, -5.2096e-01, -1.9994e-02, -2.5678e-01,\n",
      "         -1.4060e-01, -2.2920e-01, -3.0817e-01,  1.4373e-01, -1.2202e-01,\n",
      "         -7.7573e-02, -5.3782e-02,  2.2136e-01,  7.0203e-02, -1.8093e-01,\n",
      "         -8.0397e-02,  1.2206e-02, -1.5001e-01,  3.7048e-02,  8.4631e-02,\n",
      "         -1.2181e-01, -3.2780e-01,  1.2084e-01, -2.1678e-01,  4.3061e-01,\n",
      "         -6.2987e-02,  1.9309e-01, -6.8173e-02,  5.7889e-01,  4.3552e-01,\n",
      "         -8.4922e-02,  1.7860e-01,  1.4578e-02, -2.9688e-01,  3.5936e-01,\n",
      "         -2.1212e-01,  2.8721e-01, -1.8373e-01,  5.7352e-02, -7.4020e-02,\n",
      "          2.5149e-01,  1.2284e-02, -3.2084e-01,  4.7319e-03, -5.0427e-01,\n",
      "         -3.4603e-02, -2.5372e-01, -3.2277e-01, -1.1014e-01, -6.9819e-02,\n",
      "          5.1173e-02,  3.2351e-01,  4.6701e-01, -3.5659e-01,  1.3945e-01,\n",
      "         -1.8463e-01, -2.3453e-01, -1.2766e-01, -2.0427e-01,  1.4016e-02,\n",
      "         -2.0515e-01,  3.4501e-01,  1.6569e-02,  1.8262e-01,  3.5885e-01,\n",
      "         -3.3782e-03, -3.4956e-01,  2.6173e-01, -2.7241e-01,  2.7160e-01,\n",
      "          1.8464e-01, -5.6260e-02, -1.2192e-01, -8.3377e-02,  3.3390e-01,\n",
      "          5.0792e-01, -1.4366e-01, -9.0465e-02,  2.5608e-01, -2.6847e-01,\n",
      "          4.6218e-01, -4.4063e-01,  6.1194e-02,  1.7520e-02, -2.8180e-03,\n",
      "          1.7718e-01,  3.3491e-01,  1.5747e-01,  2.1664e-01,  3.8083e-01,\n",
      "         -1.8314e-01,  2.6835e-02, -2.7013e-01,  4.3851e-01, -1.4414e-01,\n",
      "         -1.8099e-01,  2.0057e-01,  6.2111e-02, -5.0461e-02, -1.0425e-01,\n",
      "         -1.8820e-01, -7.5753e-02, -5.6948e-02, -3.1079e-01, -1.4227e-02,\n",
      "          1.9583e-01, -6.3116e-01, -7.5822e-03, -1.0566e-01,  1.3622e-01,\n",
      "          2.7556e-01,  2.9097e-01,  1.5383e-01,  2.8610e-01,  1.5620e-01,\n",
      "          3.1474e-01,  6.0631e-02,  1.7065e-01,  4.6408e-01,  1.1037e-01,\n",
      "          2.3562e-01, -5.1005e-01,  1.4691e-01,  1.3688e-01,  9.7482e-02,\n",
      "         -7.1767e-02, -3.1803e-01,  6.0243e-02,  4.4972e-01, -9.2100e-02,\n",
      "         -1.4711e-01,  4.7971e-02, -3.8997e-01,  2.9957e-02, -4.3791e-02,\n",
      "         -5.9794e-02, -1.7768e-01, -1.3875e-01,  1.0848e-01,  2.4844e-01,\n",
      "          1.7589e-01, -4.8479e-02, -3.9551e-01,  2.2526e-02,  1.1957e-01,\n",
      "          8.3678e-02,  9.4242e-02,  1.0455e-01, -2.5883e-01, -1.5901e-01,\n",
      "          5.0230e-01, -1.6780e-01, -1.7744e-01,  2.8317e-02,  1.2783e-01,\n",
      "         -1.8800e-01, -8.2084e-03, -5.1583e-02, -4.0490e-01,  2.3644e-01,\n",
      "         -1.2281e-01, -3.4157e-02,  8.0385e-02,  7.3885e-02, -3.7622e-02,\n",
      "         -1.9939e-01,  2.9306e-01, -3.5621e-02,  1.5102e-01, -1.5255e-01,\n",
      "         -9.2336e-02, -1.3988e-01,  3.0829e-02,  2.1946e-01,  5.5531e-02,\n",
      "         -9.9383e-02, -3.2589e-02, -3.7024e-01,  3.8528e-02,  2.9234e-01,\n",
      "         -2.5009e-01, -2.0158e-01, -2.9504e-02, -2.7807e-01,  4.0415e-01,\n",
      "          1.5820e-01, -2.0417e-01, -3.4660e-02,  1.1838e-03, -2.2806e-01,\n",
      "          2.9226e-01, -4.0868e-01, -2.2090e-01, -4.1762e-01, -9.7994e-03,\n",
      "          7.5591e-02, -4.4760e-01, -1.1464e-01,  4.9511e-01, -2.4689e-01,\n",
      "          1.3165e-01,  1.6945e-02,  4.0434e-03, -2.8690e-01,  1.4638e-01,\n",
      "         -2.0070e-01,  1.0614e-01,  9.1754e-02,  1.0373e-01,  6.5804e-02,\n",
      "         -1.0371e-01, -1.7721e-01, -1.3976e-01,  2.9851e-02, -5.3978e-02,\n",
      "          5.4038e-01, -5.7668e-01,  2.5764e-01,  7.0658e-02, -1.7092e-01,\n",
      "         -1.1593e-01,  7.7007e-02, -4.4933e-02,  7.6154e-02, -1.8593e-01,\n",
      "          1.9429e-01,  2.1242e-01,  1.8901e-01,  1.8369e-03, -2.4499e-01,\n",
      "         -1.7324e-01,  4.7939e-01,  5.4526e-02, -3.3339e-02, -1.1862e-01,\n",
      "          9.4983e-02,  6.6461e-02,  4.1979e-01,  3.2154e-02, -1.9462e-01,\n",
      "          2.0795e-01, -2.1634e-01,  3.5522e-02,  7.3987e-03,  1.4602e-02,\n",
      "         -1.4272e-01,  2.6935e-01,  7.0660e-02, -1.2677e-01, -2.6150e-01,\n",
      "          1.0660e-01,  1.8830e-01,  5.5407e-02,  5.1442e-01, -8.4582e-02,\n",
      "         -2.7588e-01, -4.7336e-01, -3.2759e-01,  2.9165e-01, -1.8952e-01,\n",
      "          2.0927e-01, -1.6864e-01,  6.6181e-02,  3.3892e-01,  5.5557e-02,\n",
      "          1.8023e-01, -2.1340e-02, -1.0050e-01,  9.8507e-02,  3.5184e-01,\n",
      "          3.1227e-02,  5.3343e-03,  5.2818e-03,  1.3326e-01,  1.6992e-01,\n",
      "         -1.3922e-01, -1.5000e-01, -7.6867e-02, -2.6409e-01, -5.4694e-01,\n",
      "         -1.2321e-01,  6.8454e-04, -5.8403e-02, -4.4109e-02,  2.8532e-01,\n",
      "          7.8533e-03, -5.2920e-02, -6.3690e-02,  2.4183e-01,  9.0064e-02,\n",
      "          1.5840e-01, -1.0560e-01, -2.7902e-01, -6.0878e-02, -2.5268e-02,\n",
      "         -3.1908e-01, -1.8301e-01,  5.4615e-01, -8.8715e-02, -8.4439e-02,\n",
      "          2.9724e-01, -2.5875e-02,  4.1716e-02,  1.4826e-01, -3.0507e-01,\n",
      "          4.1095e-01, -5.2381e-01,  2.4453e-01,  2.0494e-01,  7.2133e-02,\n",
      "          4.7586e-01, -2.5451e-02,  2.0528e-01,  3.5608e-01,  6.9454e-02,\n",
      "         -2.9865e-01,  1.8934e-03,  1.5255e-01, -1.3087e-02, -2.0092e-01,\n",
      "         -1.8651e-01, -4.8009e-04,  3.3971e-01,  3.2517e-01,  1.1711e-01,\n",
      "          3.3301e-01, -3.6459e-01,  2.1778e-01,  1.4183e-02, -2.7648e-01,\n",
      "         -3.0335e-01, -1.6830e-01,  2.5042e-01,  3.0473e-01,  3.8968e-01,\n",
      "         -4.6454e-02, -4.1688e-01,  1.0371e-01, -5.6405e-02, -2.0507e-01,\n",
      "          9.2274e-02, -4.7247e-01, -1.7958e-01, -3.0366e-01, -6.0407e-02,\n",
      "         -3.5432e-01,  6.9053e-02, -2.4498e-01, -5.8493e-02,  3.8413e-01,\n",
      "         -1.2800e-01, -3.1178e-01,  1.5353e-01, -1.4998e-01,  2.8685e-01,\n",
      "         -2.7571e-02,  2.1282e-01,  2.4398e-01,  1.5368e-01, -5.5408e-03,\n",
      "          9.8986e-03, -1.7705e-01,  2.4659e-01, -1.4658e-01,  2.2906e-01,\n",
      "          4.5143e-01,  7.2585e-02, -2.9133e-01,  2.1401e-01, -7.3459e-02,\n",
      "         -2.2381e-01, -3.8480e-01, -3.6433e-02, -8.6398e-02,  1.4917e-01,\n",
      "          1.0637e-01, -1.3652e-01,  2.3579e-01,  8.5476e-02, -2.8537e-02,\n",
      "         -1.9751e-01,  6.8070e-02,  3.4997e-01, -1.5211e-02, -9.5495e-02,\n",
      "         -4.3755e-01, -3.4468e-01, -3.0454e-01,  3.0778e-01,  4.6703e-02,\n",
      "         -3.9421e-02,  1.7246e-01,  2.5148e-01,  1.1320e-02,  2.1263e-01,\n",
      "         -1.7949e-01,  1.1550e-01,  1.2214e-01,  6.5860e-02,  3.6955e-02,\n",
      "          2.5363e-02,  8.6028e-02, -1.3920e-02, -4.6333e-04,  2.1788e-01,\n",
      "         -5.4112e-02, -2.4934e-02,  2.3047e-01, -7.5713e-02,  3.1664e-01,\n",
      "         -2.6292e-01,  2.3376e-01, -1.3918e-02, -8.5672e-02,  4.1563e-01,\n",
      "         -2.5399e-02,  1.6480e-01, -1.6229e-01, -1.1191e-01, -3.3496e-02,\n",
      "          5.5672e-01, -2.0177e-02,  7.8113e-02, -1.0054e-01,  5.4457e-01,\n",
      "         -1.0392e-01, -2.5417e-01,  4.0717e-02, -4.0233e-01, -7.5007e-02,\n",
      "         -1.8427e-01, -1.0257e-01, -5.1813e-01, -1.0679e-01,  3.5077e-01,\n",
      "         -9.4850e-02,  3.3760e-01, -1.6847e-01,  1.4390e-01,  6.1594e-01,\n",
      "          8.3343e-02, -2.3057e-01, -1.0495e-02, -2.9088e-01,  2.0451e-01,\n",
      "          2.3600e-01, -3.2576e-01, -6.2675e-02, -4.6358e-01,  1.1758e-01,\n",
      "          1.4671e-01, -1.7518e-01,  1.0243e-01,  1.4045e-01,  1.5160e-01,\n",
      "         -5.4363e-02,  1.9278e-01, -2.6887e-01,  7.0766e-03, -1.1655e-01,\n",
      "         -2.1273e-02, -1.5818e-01,  4.5235e-02, -2.0372e-01, -4.9776e-02,\n",
      "          1.6901e-02,  2.3563e-01, -2.0951e-01, -1.1261e-01,  6.1015e-02,\n",
      "          3.6121e-01, -1.2800e-02, -3.7760e-01,  2.7989e-02,  1.8727e-01,\n",
      "         -4.0425e-01,  3.2005e-02, -4.3698e-01, -2.4938e-01,  4.0457e-01,\n",
      "         -1.3259e-01, -1.0918e-01,  4.7613e-02, -2.2403e-01, -2.8775e-01,\n",
      "          9.3284e-02, -5.0052e-02, -3.5897e-01, -3.9017e-01,  5.9657e-02,\n",
      "         -2.3092e-01,  1.8464e-03,  1.2932e-01, -3.6175e-01, -5.4553e-02,\n",
      "          1.3584e-01,  4.3913e-01, -9.5812e-02, -1.8803e-01,  1.5401e-01,\n",
      "         -2.2958e-01,  2.1124e-01, -5.7872e-03,  5.7403e-01, -3.0717e-01,\n",
      "         -2.9392e-01,  4.7485e-01, -4.1326e-01, -2.8510e-01, -2.2736e-01,\n",
      "         -2.1375e-02,  1.1518e-01, -8.0182e-02,  2.2486e-01, -4.1750e-01]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#%% \n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch._utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "# sys.path.append('/home/ubuntu/ds/segmentation/HRNet-Semantic-Segmentation/lib/models/')\n",
    "\n",
    "# from bn_helper import BatchNorm2d, BatchNorm2d_class, relu_inplace\n",
    "\n",
    "ALIGN_CORNERS = True\n",
    "BN_MOMENTUM = 0.1\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ModuleHelper:\n",
    "\n",
    "    @staticmethod\n",
    "    def BNReLU(num_features, bn_type=None, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            BatchNorm2d(num_features, **kwargs),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def BatchNorm2d(*args, **kwargs):\n",
    "        return BatchNorm2d\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "class SpatialGather_Module(nn.Module):\n",
    "    \"\"\"\n",
    "        Aggregate the context features according to the initial \n",
    "        predicted probability distribution.\n",
    "        Employ the soft-weighted method to aggregate the context.\n",
    "    \"\"\"\n",
    "    def __init__(self, cls_num=0, scale=1):\n",
    "        super(SpatialGather_Module, self).__init__()\n",
    "        self.cls_num = cls_num\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, feats, probs):\n",
    "        batch_size, c, h, w = probs.size(0), probs.size(1), probs.size(2), probs.size(3)\n",
    "        probs = probs.view(batch_size, c, -1)\n",
    "        feats = feats.view(batch_size, feats.size(1), -1)\n",
    "        feats = feats.permute(0, 2, 1) # batch x hw x c \n",
    "        probs = F.softmax(self.scale * probs, dim=2)# batch x k x hw\n",
    "        ocr_context = torch.matmul(probs, feats)\\\n",
    "        .permute(0, 2, 1).unsqueeze(3)# batch x k x c\n",
    "        return ocr_context\n",
    "\n",
    "\n",
    "class _ObjectAttentionBlock(nn.Module):\n",
    "    '''\n",
    "    The basic implementation for object context block\n",
    "    Input:\n",
    "        N X C X H X W\n",
    "    Parameters:\n",
    "        in_channels       : the dimension of the input feature map\n",
    "        key_channels      : the dimension after the key/query transform\n",
    "        scale             : choose the scale to downsample the input feature maps (save memory cost)\n",
    "        bn_type           : specify the bn type\n",
    "    Return:\n",
    "        N X C X H X W\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 key_channels, \n",
    "                 scale=1, \n",
    "                 bn_type=None):\n",
    "        super(_ObjectAttentionBlock, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n",
    "        self.f_pixel = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n",
    "            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n",
    "        )\n",
    "        self.f_object = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n",
    "            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n",
    "        )\n",
    "        self.f_down = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.key_channels, bn_type=bn_type),\n",
    "        )\n",
    "        self.f_up = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,\n",
    "                kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(self.in_channels, bn_type=bn_type),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, proxy):\n",
    "        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n",
    "        if self.scale > 1:\n",
    "            x = self.pool(x)\n",
    "\n",
    "        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)\n",
    "        query = query.permute(0, 2, 1)\n",
    "        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)\n",
    "        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)\n",
    "        value = value.permute(0, 2, 1)\n",
    "\n",
    "        sim_map = torch.matmul(query, key)\n",
    "        sim_map = (self.key_channels**-.5) * sim_map\n",
    "        sim_map = F.softmax(sim_map, dim=-1)   \n",
    "\n",
    "        # add bg context ...\n",
    "        context = torch.matmul(sim_map, value)\n",
    "        context = context.permute(0, 2, 1).contiguous()\n",
    "        context = context.view(batch_size, self.key_channels, *x.size()[2:])\n",
    "        context = self.f_up(context)\n",
    "        if self.scale > 1:\n",
    "            context = F.interpolate(input=context, size=(h, w), mode='bilinear', align_corners=ALIGN_CORNERS)\n",
    "\n",
    "        return context\n",
    "\n",
    "class ObjectAttentionBlock2D(_ObjectAttentionBlock):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 key_channels, \n",
    "                 scale=1, \n",
    "                 bn_type=None):\n",
    "        super(ObjectAttentionBlock2D, self).__init__(in_channels,\n",
    "                                                     key_channels,\n",
    "                                                     scale, \n",
    "                                                     bn_type=bn_type)\n",
    "\n",
    "\n",
    "class SpatialOCR_Module(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the OCR module:\n",
    "    We aggregate the global object representation to update the representation for each pixel.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 key_channels, \n",
    "                 out_channels, \n",
    "                 scale=1, \n",
    "                 dropout=0.1, \n",
    "                 bn_type=None):\n",
    "        super(SpatialOCR_Module, self).__init__()\n",
    "        self.object_context_block = ObjectAttentionBlock2D(in_channels, \n",
    "                                                           key_channels, \n",
    "                                                           scale, \n",
    "                                                           bn_type)\n",
    "        _in_channels = 2 * in_channels\n",
    "\n",
    "        self.conv_bn_dropout = nn.Sequential(\n",
    "            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n",
    "            ModuleHelper.BNReLU(out_channels, bn_type=bn_type),\n",
    "            nn.Dropout2d(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, feats, proxy_feats):\n",
    "        context = self.object_context_block(feats, proxy_feats)\n",
    "\n",
    "        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n",
    "\n",
    "        return output\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride, padding=1, bias=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=nn.BatchNorm2d):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, stride, 1, bias=False)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # 下面层的目的是： 如果bottleneck的输入，输出的channel的数量相同，就不需要如下的两个层，直接连接即可\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class HighResolutionModule(nn.Module):\n",
    "    def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels,\n",
    "                 fuse_method, multi_scale_output=True, norm_layer=nn.BatchNorm2d):\n",
    "        super(HighResolutionModule, self).__init__()\n",
    "        assert num_branches == len(num_blocks)\n",
    "        assert num_branches == len(num_channels)\n",
    "        assert num_branches == len(num_inchannels)\n",
    "\n",
    "        self.num_inchannels = num_inchannels\n",
    "        self.fuse_method = fuse_method\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        self.multi_scale_output = multi_scale_output\n",
    "\n",
    "        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels, norm_layer=norm_layer)\n",
    "        self.fuse_layers = self._make_fuse_layers(norm_layer)\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n",
    "                         stride=1, norm_layer=nn.BatchNorm2d):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion,\n",
    "                          1, stride, bias=False),\n",
    "                norm_layer(num_channels[branch_index] * block.expansion))\n",
    "\n",
    "        layers = list()\n",
    "        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index],\n",
    "                            stride, downsample, norm_layer=norm_layer))\n",
    "        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n",
    "        for i in range(1, num_blocks[branch_index]):\n",
    "            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_branches(self, num_branches, block, num_blocks, num_channels, norm_layer=nn.BatchNorm2d):\n",
    "        branches = list()\n",
    "        for i in range(num_branches):\n",
    "            branches.append(\n",
    "                self._make_one_branch(i, block, num_blocks, num_channels, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.ModuleList(branches)\n",
    "\n",
    "    def _make_fuse_layers(self, norm_layer=nn.BatchNorm2d):\n",
    "        if self.num_branches == 1:\n",
    "            return None\n",
    "\n",
    "        num_branches = self.num_branches\n",
    "        num_inchannels = self.num_inchannels\n",
    "        fuse_layers = []\n",
    "        for i in range(num_branches if self.multi_scale_output else 1):\n",
    "            fuse_layer = list()\n",
    "            for j in range(num_branches):\n",
    "                if j > i:\n",
    "                    fuse_layer.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, bias=False),\n",
    "                        norm_layer(num_inchannels[i]),\n",
    "                        nn.Upsample(scale_factor=2 ** (j - i), mode='nearest')))\n",
    "                elif j == i:\n",
    "                    fuse_layer.append(None)\n",
    "                else:\n",
    "                    conv3x3s = list()\n",
    "                    for k in range(i - j):\n",
    "                        if k == i - j - 1:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[i]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n",
    "                                norm_layer(num_outchannels_conv3x3)))\n",
    "                        else:\n",
    "                            num_outchannels_conv3x3 = num_inchannels[j]\n",
    "                            conv3x3s.append(nn.Sequential(\n",
    "                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n",
    "                                norm_layer(num_outchannels_conv3x3),\n",
    "                                nn.ReLU(False)))\n",
    "                    fuse_layer.append(nn.Sequential(*conv3x3s))\n",
    "            fuse_layers.append(nn.ModuleList(fuse_layer))\n",
    "\n",
    "        return nn.ModuleList(fuse_layers)\n",
    "\n",
    "    def get_num_inchannels(self):\n",
    "        return self.num_inchannels\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.num_branches == 1:\n",
    "            return [self.branches[0](x[0])]\n",
    "\n",
    "        for i in range(self.num_branches):\n",
    "            x[i] = self.branches[i](x[i])\n",
    "\n",
    "        x_fuse = list()\n",
    "        for i in range(len(self.fuse_layers)):\n",
    "            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n",
    "            for j in range(1, self.num_branches):\n",
    "                if i == j:\n",
    "                    y = y + x[j]\n",
    "                elif j > i:\n",
    "                    width_output = x[i].shape[-1]\n",
    "                    height_output = x[i].shape[-2]\n",
    "                    y = y + F.interpolate(\n",
    "                        self.fuse_layers[i][j](x[j]),\n",
    "                        size=[height_output, width_output],\n",
    "                        mode='bilinear', align_corners=ALIGN_CORNERS)\n",
    "                else:\n",
    "                    y = y + self.fuse_layers[i][j](x[j])\n",
    "            x_fuse.append(self.relu(y))\n",
    "\n",
    "        return x_fuse\n",
    "\n",
    "\n",
    "class HighResolutionNet(nn.Module):\n",
    "    def __init__(self, blocks, num_channels, num_modules, num_branches, num_blocks,\n",
    "                 fuse_method, norm_layer=nn.BatchNorm2d, **kwargs):\n",
    "        super(HighResolutionNet, self).__init__()\n",
    "        self.num_branches = num_branches\n",
    "\n",
    "        # deep stem\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 2, 1, bias=False),\n",
    "            norm_layer(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, 3, 2, 1, bias=False),\n",
    "            norm_layer(64),\n",
    "            nn.ReLU(True))\n",
    "        \n",
    "        # stage 1\n",
    "\n",
    "        self.layer1 = self._make_layer(Bottleneck, 64, 64, 4, norm_layer=norm_layer)\n",
    "        stage1_out_channel = 256\n",
    "\n",
    "        # stage 2\n",
    "        num_channel, block = num_channels[0], blocks[0]\n",
    "        channels = [channel * block.expansion for channel in num_channel]\n",
    "        self.transition1 = self._make_transition_layer([stage1_out_channel], channels, norm_layer)\n",
    "        self.stage2, pre_stage_channels = self._make_stage(num_modules[0], num_branches[0],\n",
    "                                                           num_blocks[0], channels, block,\n",
    "                                                           fuse_method[0], channels,\n",
    "                                                           norm_layer=norm_layer)\n",
    "\n",
    "        # stage 3\n",
    "        num_channel, block = num_channels[1], blocks[1]\n",
    "        channels = [channel * block.expansion for channel in num_channel]\n",
    "        self.transition2 = self._make_transition_layer(pre_stage_channels, channels, norm_layer)\n",
    "        self.stage3, pre_stage_channels = self._make_stage(num_modules[1], num_branches[1],\n",
    "                                                           num_blocks[1], channels, block,\n",
    "                                                           fuse_method[1], channels,\n",
    "                                                           norm_layer=norm_layer)\n",
    "\n",
    "        # stage 4\n",
    "        num_channel, block = num_channels[2], blocks[2]\n",
    "        channels = [channel * block.expansion for channel in num_channel]\n",
    "        self.transition3 = self._make_transition_layer(pre_stage_channels, channels, norm_layer)\n",
    "        self.stage4, pre_stage_channels = self._make_stage(num_modules[2], num_branches[2],\n",
    "                                                           num_blocks[2], channels, block,\n",
    "                                                           fuse_method[2], channels,\n",
    "                                                           norm_layer=norm_layer)\n",
    "        # FIXME: OCR\n",
    "        last_inp_channels = np.int(np.sum(pre_stage_channels))\n",
    "        # ocr_mid_channels = config.MODEL.OCR.MID_CHANNELS\n",
    "        ocr_mid_channels = 512\n",
    "        # ocr_key_channels = config.MODEL.OCR.KEY_CHANNELS       \n",
    "        ocr_key_channels = 256\n",
    "        self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(pre_stage_channels, norm_layer)\n",
    "        self.classifier = nn.Linear(2048, 1000)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, stride=1, norm_layer=nn.BatchNorm2d):\n",
    "        downsample = None\n",
    "        if stride != 1 or inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, planes * block.expansion, 1, stride, bias=False),\n",
    "                norm_layer(planes * block.expansion))\n",
    "\n",
    "        layers = list()\n",
    "        layers.append(block(inplanes, planes, stride, downsample=downsample, norm_layer=norm_layer))\n",
    "        inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(inplanes, planes, norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer, norm_layer=nn.BatchNorm2d):\n",
    "        # 首先获取当前分支与前一个分支的数目\n",
    "        num_branches_cur = len(num_channels_cur_layer)\n",
    "        num_branches_pre = len(num_channels_pre_layer)\n",
    "\n",
    "        transition_layers = list()\n",
    "        for i in range(num_branches_cur):\n",
    "            if i < num_branches_pre:\n",
    "                # 首先处理当前分支中存在的前面的几层\n",
    "                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n",
    "                    # 通道数目不同的时候，需要用311修改通道数目，256-->32\n",
    "                    transition_layers.append(nn.Sequential(\n",
    "                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, padding=1, bias=False),\n",
    "                        norm_layer(num_channels_cur_layer[i]),\n",
    "                        nn.ReLU(True)))\n",
    "                else:\n",
    "                    transition_layers.append(None)\n",
    "            else:\n",
    "                #  i>=num_branches_pre,已经处理完成包含的前面分支的数据，开始处理后面独有的新增的部分，使用321进行降维处理，256-->64\n",
    "                conv3x3s = list()\n",
    "                for j in range(i + 1 - num_branches_pre):\n",
    "                    in_channels = num_channels_pre_layer[-1]\n",
    "                    out_channels = num_channels_cur_layer[i] if j == i - num_branches_pre else in_channels\n",
    "                    conv3x3s.append(nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, 3, 2, 1, bias=False),\n",
    "                        norm_layer(out_channels),\n",
    "                        nn.ReLU(True)))\n",
    "                transition_layers.append(nn.Sequential(*conv3x3s))\n",
    "\n",
    "        return nn.ModuleList(transition_layers)\n",
    "\n",
    "    def _make_stage(self, num_modules, num_branches, num_blocks, num_channels, block,\n",
    "                    fuse_method, num_inchannels, multi_scale_output=True, norm_layer=nn.BatchNorm2d):\n",
    "        modules = list()\n",
    "        for i in range(num_modules):\n",
    "            # multi_scale_output is only used last module\n",
    "            if not multi_scale_output and i == num_modules - 1:\n",
    "                reset_multi_scale_output = False\n",
    "            else:\n",
    "                reset_multi_scale_output = True\n",
    "\n",
    "            modules.append(HighResolutionModule(num_branches, block, num_blocks, num_inchannels, num_channels,\n",
    "                                                fuse_method, reset_multi_scale_output, norm_layer=norm_layer))\n",
    "            num_inchannels = modules[-1].get_num_inchannels()\n",
    "\n",
    "        return nn.Sequential(*modules), num_inchannels\n",
    "\n",
    "    def _make_head(self, pre_stage_channels, norm_layer=nn.BatchNorm2d):\n",
    "        head_block = Bottleneck\n",
    "        head_channels = [32, 64, 128, 256]\n",
    "\n",
    "        # Increasing the #channels on each resolution\n",
    "        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n",
    "        incre_modules = list()\n",
    "        for i, channels in enumerate(pre_stage_channels):\n",
    "            incre_module = self._make_layer(head_block, channels, head_channels[i], 1)\n",
    "            incre_modules.append(incre_module)\n",
    "        incre_modules = nn.ModuleList(incre_modules)\n",
    "\n",
    "        # downsampling modules\n",
    "        downsamp_modules = []\n",
    "        for i in range(len(pre_stage_channels) - 1):\n",
    "            in_channels = head_channels[i] * head_block.expansion\n",
    "            out_channels = head_channels[i + 1] * head_block.expansion\n",
    "\n",
    "            downsamp_module = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, 2, 1),\n",
    "                norm_layer(out_channels),\n",
    "                nn.ReLU(True))\n",
    "\n",
    "            downsamp_modules.append(downsamp_module)\n",
    "        downsamp_modules = nn.ModuleList(downsamp_modules)\n",
    "\n",
    "        final_layer = nn.Sequential(\n",
    "            nn.Conv2d(head_channels[3] * head_block.expansion, 2048, 1),\n",
    "            norm_layer(2048),\n",
    "            nn.ReLU(True))\n",
    "\n",
    "        return incre_modules, downsamp_modules, final_layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # deep stem\n",
    "        x = self.conv1(x)\n",
    "        # stage1 start  开始组合多个stage的部分，每个stage核心是调用HighResolutionModule模块\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        x_list = list()\n",
    "        \n",
    "        for i in range(self.num_branches[0]):\n",
    "            if self.transition1[i] is not None:\n",
    "                tmp = self.transition1[i](x)\n",
    "                print(tmp.size())\n",
    "                x_list.append(self.transition1[i](x))\n",
    "            else:\n",
    "                x_list.append(x)\n",
    "        y_list = self.stage2(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.num_branches[1]):\n",
    "            if self.transition2[i] is not None:\n",
    "                x_list.append(self.transition2[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage3(x_list)\n",
    "\n",
    "        x_list = []\n",
    "        for i in range(self.num_branches[2]):\n",
    "            if self.transition3[i] is not None:\n",
    "                x_list.append(self.transition3[i](y_list[-1]))\n",
    "            else:\n",
    "                x_list.append(y_list[i])\n",
    "        y_list = self.stage4(x_list)\n",
    "\n",
    "        # Classification Head\n",
    "        y = self.incre_modules[0](y_list[0])\n",
    "        for i in range(len(self.downsamp_modules)):\n",
    "            y = self.incre_modules[i + 1](y_list[i + 1]) + self.downsamp_modules[i](y)\n",
    "\n",
    "        y = self.final_layer(y)\n",
    "\n",
    "        y = F.avg_pool2d(y, kernel_size=y.size()\n",
    "        [2:]).view(y.size(0), -1)\n",
    "\n",
    "        y = self.classifier(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "# blocks = [BasicBlock, BasicBlock, BasicBlock]\n",
    "# num_modules = [1, 1, 1]\n",
    "# num_branches = [2, 3, 4]\n",
    "# num_blocks = [[4, 4], [4, 4, 4], [4, 4, 4, 4]]\n",
    "# num_channels = [[256, 256], [32, 64, 128], [32, 64, 128, 256]]\n",
    "# fuse_method = ['sum', 'sum', 'sum']\n",
    "\n",
    "blocks = [BasicBlock, BasicBlock, BasicBlock]\n",
    "num_modules = [1, 1, 1]\n",
    "num_branches = [2, 3, 4]\n",
    "num_blocks = [[4, 4], [4, 4, 4], [4, 4, 4, 4]]\n",
    "# num_channels format: [stage1[channel1,channel2,channel3],stage2...]\n",
    "num_channels = [[48, 96], [48, 96, 192], [48, 96, 192, 384]]\n",
    "fuse_method = ['sum', 'sum', 'sum']\n",
    "\n",
    "#%% \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = torch.randn(1, 3, 512, 512)\n",
    "    model = HighResolutionNet(blocks, num_channels, num_modules, num_branches, num_blocks, fuse_method)\n",
    "    output = model(img)\n",
    "    print(output)\n",
    "\n",
    "\n",
    "# %%\n",
    "for i in range(3):\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('fastai2': conda)",
   "language": "python",
   "name": "python37564bitfastai2conda0460244ff620488f9ecf7aa8887aa15c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
